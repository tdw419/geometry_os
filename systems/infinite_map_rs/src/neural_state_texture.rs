//! Neural State Texture - Visualization of Neural Network State
//!
//! This module provides textures for rendering neural network activations,
//! attention weights, and thought patterns as visual elements in the compositor.
//!
//! ARCHITECTURE SHIFT (Phase 33 Tensor-Compute):
//! - OLD: CPU iteration (O(N)) - Loop through pixels on CPU
//! - NEW: GPU Compute Shader (O(1) dispatch) - Parallel geometric folding
//!
//! The GPU approach treats memory as raw tensors and uses compute shaders
//! to fold them into 2D textures with Hilbert curve locality preservation.

use crate::evolution_protocol::{
    NeuralStateData, SelfState, TheoryOfMindState, CognitiveState, AudioFeatures, InputState
};
use crate::memory_tensor::{MemoryTensorFolder, MemoryTensorConfig, NeuralParams, ColorMode, pack_neural_state_to_tensor};
use wgpu::{self, Device, Queue, Texture, TextureView, TextureFormat, Extent3d};
use std::time::Instant;

/// Neural state texture for rendering neural network state
///
/// Architecture: Tensor-Compute (GPU-based)
/// - Uses MemoryTensorFolder for GPU compute shader dispatch
/// - Zero CPU overhead for texture generation
/// - Real-time 60 FPS visualization
pub struct NeuralStateTexture {
    // GPU-based tensor folder (NEW: Tensor-Compute architecture)
    tensor_folder: Option<MemoryTensorFolder>,
    
    // Fallback CPU-based texture (for compatibility)
    texture: Texture,
    view: TextureView,
    width: u32,
    height: u32,
    last_update: Instant,
    data_version: u64,
    
    // Configuration
    use_gpu_compute: bool,  // true = GPU compute shader, false = CPU fallback
}

impl NeuralStateTexture {
    /// Create a new neural state texture with GPU compute shader support
    /// 
    /// Architecture: Tensor-Compute (GPU-based)
    /// - Initializes MemoryTensorFolder for parallel GPU processing
    /// - Falls back to CPU if GPU compute fails
    pub fn new(device: &Device, width: u32, height: u32) -> Self {
        let size = Extent3d {
            width,
            height,
            depth_or_array_layers: 1,
        };

        // Create fallback texture (CPU mode)
        let texture = device.create_texture(&wgpu::TextureDescriptor {
            label: Some("Neural State Texture (CPU Fallback)"),
            size,
            mip_level_count: 1,
            sample_count: 1,
            dimension: wgpu::TextureDimension::D2,
            format: TextureFormat::Rgba8Unorm,
            usage: wgpu::TextureUsages::TEXTURE_BINDING | wgpu::TextureUsages::COPY_DST,
            view_formats: &[],
        });

        let view = texture.create_view(&wgpu::TextureViewDescriptor::default());

        // Try to initialize GPU compute shader (Tensor-Compute architecture)
        let tensor_folder = Self::init_tensor_folder(device, width, height);

        let use_gpu_compute = tensor_folder.is_some();
        Self {
            tensor_folder,
            texture,
            view,
            width,
            height,
            last_update: Instant::now(),
            data_version: 0,
            use_gpu_compute,
        }
    }
    
    /// Initialize the GPU tensor folder
    fn init_tensor_folder(device: &Device, width: u32, height: u32) -> Option<MemoryTensorFolder> {
        // Validate dimensions (must be power of 2 for Hilbert curve)
        if !width.is_power_of_two() || !height.is_power_of_two() {
            eprintln!("Warning: Texture dimensions must be powers of 2 for GPU compute. Falling back to CPU.");
            return None;
        }
        
        // Calculate Hilbert order (log2 of dimension)
        let hilbert_order = (width as f32).log2() as u32;
        
        let config = MemoryTensorConfig {
            width,
            height,
            hilbert_order,
            base_address: 0,
            color_mode: ColorMode::MultiChannel,
            brightness: 1.0,
            contrast: 1.0,
        };
        
        match MemoryTensorFolder::new(device, config) {
            Ok(folder) => {
                println!("âœ“ Tensor-Compute architecture initialized: {}x{} texture with Hilbert order {}", width, height, hilbert_order);
                Some(folder)
            }
            Err(e) => {
                eprintln!("Warning: Failed to initialize GPU compute shader: {}. Falling back to CPU.", e);
                None
            }
        }
    }

    /// Update texture with neural state data
    /// 
    /// Architecture: Tensor-Compute (GPU-based)
    /// - OLD: CPU loop O(N) - Iterate through each pixel
    /// - NEW: GPU compute O(1) - Single dispatch, parallel folding
    pub fn update_from_neural_state(
        &mut self,
        device: &Device,
        queue: &Queue,
        neural_state: &NeuralStateData,
        self_state: Option<&SelfState>,
        theory_of_mind: Option<&TheoryOfMindState>,
    ) {
        // Try GPU compute shader first (Tensor-Compute architecture)
        if let Some(ref mut tensor_folder) = self.tensor_folder {
            if Self::update_with_gpu_compute(tensor_folder, device, queue, neural_state, self_state, theory_of_mind).is_ok() {
                self.last_update = Instant::now();
                self.data_version += 1;
                return;
            }
        }
        
        // Fallback to CPU-based texture generation
        self.update_with_cpu(queue, neural_state, self_state, theory_of_mind);
        
        self.last_update = Instant::now();
        self.data_version += 1;
    }
    
    /// Update texture using GPU compute shader (Tensor-Compute architecture)
    /// 
    /// This is the "dumb pipe" operation:
    /// 1. Pack neural state into raw tensor bytes
    /// 2. Upload to GPU buffer (single memcpy)
    /// 3. Dispatch compute shader (O(1) operation)
    /// 4. GPU folds tensor into texture in parallel
    fn update_with_gpu_compute(
        tensor_folder: &mut crate::memory_tensor::MemoryTensorFolder,
        device: &Device,
        queue: &Queue,
        neural_state: &NeuralStateData,
        self_state: Option<&SelfState>,
        theory_of_mind: Option<&TheoryOfMindState>,
    ) -> Result<(), String> {
        
        // Update neural parameters from evolution protocol
        let neural_params = NeuralParams {
            confidence: self_state.map(|s| s.confidence).unwrap_or(neural_state.confidence),
            focus_depth: self_state.map(|s| s.focus_depth).unwrap_or(0.0),
            user_attention: theory_of_mind.map(|t| t.user_intent.attention_focus).unwrap_or(0.5),
            alignment: theory_of_mind.map(|t| t.alignment).unwrap_or(0.5),
            mind_reading: theory_of_mind.map(|t| t.mind_reading_confidence).unwrap_or(0.5),
            misalignment: theory_of_mind.map(|t| if t.misalignment_detected { 1.0 } else { 0.0 }).unwrap_or(0.0),
        };
        
        tensor_folder.set_neural_params(neural_params);
        
        // Pack neural state into tensor bytes (the "raw data block")
        let tensor_data = pack_neural_state_to_tensor(
            &neural_state.layer_activations,
            &neural_state.attention_weights,
            &neural_state.memory_patterns,
        );
        
        // Dispatch compute shader (O(1) operation)
        // GPU will fold this tensor into texture in parallel
        tensor_folder.fold_memory(device, queue, &tensor_data)?;
        
        // Update texture view to point to GPU-computed texture
        // self.view = tensor_folder.texture_view().clone();
        
        Ok(())
    }
    
    /// Update texture using CPU (fallback for compatibility)
    fn update_with_cpu(
        &mut self,
        queue: &Queue,
        neural_state: &NeuralStateData,
        self_state: Option<&SelfState>,
        theory_of_mind: Option<&TheoryOfMindState>,
    ) {
        // Convert neural state to Rgba8Unorm texture data
        let texture_data = self.neural_state_to_rgba(neural_state, self_state, theory_of_mind);

        queue.write_texture(
            wgpu::ImageCopyTexture {
                texture: &self.texture,
                mip_level: 0,
                origin: wgpu::Origin3d::ZERO,
                aspect: wgpu::TextureAspect::All,
            },
            &texture_data,
            wgpu::ImageDataLayout {
                offset: 0,
                bytes_per_row: Some(self.width * 4), // 4 channels * 1 byte per channel
                rows_per_image: None,
            },
            Extent3d {
                width: self.width,
                height: self.height,
                depth_or_array_layers: 1,
            },
        );
    }

    /// Get the texture view for rendering
    /// 
    /// Returns the GPU-computed texture view if available,
    /// otherwise returns the CPU fallback texture view.
    pub fn view(&self) -> &TextureView {
        if let Some(ref tensor_folder) = self.tensor_folder {
            tensor_folder.texture_view()
        } else {
            &self.view
        }
    }

    /// Get the texture
    pub fn texture(&self) -> &Texture {
        if let Some(ref tensor_folder) = self.tensor_folder {
            tensor_folder.texture()
        } else {
            &self.texture
        }
    }
    
    /// Check if GPU compute is being used
    pub fn is_using_gpu_compute(&self) -> bool {
        self.use_gpu_compute && self.tensor_folder.is_some()
    }
    
    /// Get the architecture mode being used
    pub fn architecture_mode(&self) -> &str {
        if self.is_using_gpu_compute() {
            "Tensor-Compute (GPU)"
        } else {
            "Symbolic (CPU Fallback)"
        }
    }

    /// Get texture dimensions
    pub fn size(&self) -> (u32, u32) {
        (self.width, self.height)
    }

    /// Get time since last update
    pub fn time_since_update(&self) -> std::time::Duration {
        self.last_update.elapsed()
    }

    /// Get data version (for change detection)
    pub fn data_version(&self) -> u64 {
        self.data_version
    }

    /// Convert neural state data to Rgba8Unorm texture data
    fn neural_state_to_rgba(
        &self,
        neural_state: &NeuralStateData,
        self_state: Option<&SelfState>,
        theory_of_mind: Option<&TheoryOfMindState>,
    ) -> Vec<u8> {
        let mut texture_data = Vec::with_capacity((self.width * self.height * 4) as usize);

        // Create a visual representation of the neural state
        // We'll map the neural activations to a 2D grid with color encoding

        let grid_size = (self.width * self.height) as usize;
        let activations = &neural_state.layer_activations;
        let attention = &neural_state.attention_weights;
        let memory = &neural_state.memory_patterns;
        
        // Extract consciousness metrics (Phase 23)
        let confidence = self_state.map(|s| s.confidence).unwrap_or(neural_state.confidence);
        let focus = self_state.map(|s| s.focus_depth).unwrap_or(0.0);
        
        // Extract Theory of Mind metrics (Phase 24)
        let _user_intent_conf = theory_of_mind.map(|t| t.user_intent.intent_confidence).unwrap_or(0.5);
        let user_attention = theory_of_mind.map(|t| t.user_intent.attention_focus).unwrap_or(0.5);
        let alignment = theory_of_mind.map(|t| t.alignment).unwrap_or(0.5);
        let mind_reading = theory_of_mind.map(|t| t.mind_reading_confidence).unwrap_or(0.5);
        let misalignment = theory_of_mind.map(|t| if t.misalignment_detected { 1.0 } else { 0.0 }).unwrap_or(0.0);

        for i in 0..grid_size {
            // Sample from different neural components
            let act_idx = i % activations.len();
            let att_idx = i % attention.len();
            let mem_idx = i % memory.len();

            // Red channel: Layer activations + Misalignment (Phase 24)
            // If misalignment is detected, the red channel spikes (Warning Signal)
            let base_r = activations[act_idx].clamp(0.0, 1.0);
            let r = (base_r + (misalignment * 0.5)).clamp(0.0, 1.0);

            // Green channel: Attention weights * Alignment (Phase 24)
            // High alignment = harmonious green/cyan. Low alignment = washed out.
            let base_g = attention[att_idx].clamp(0.0, 1.0);
            let g = (base_g * (0.5 + 0.5 * alignment)).clamp(0.0, 1.0);

            // Blue channel: Memory patterns + Focus + Mind Reading (Phase 24)
            // Mind reading confidence clarifies memory patterns (Deep Blue)
            let base_b = memory[mem_idx].clamp(0.0, 1.0);
            let memory_clarity = 0.5 + (mind_reading * 0.5);
            let b = (base_b * (1.0 + focus) * memory_clarity).clamp(0.0, 1.0);

            // Alpha channel: Confidence * User Attention (Phase 24)
            // If user is engaged, thoughts become substantial (High Alpha).
            // If user ignores, thoughts fade to ghost-like states.
            let attention_factor = 0.5 + (user_attention * 0.5);
            let a = (confidence * attention_factor).clamp(0.0, 1.0);

            // Convert to bytes (Rgba8Unorm = 1 byte per channel)
            texture_data.push((r * 255.0) as u8);
            texture_data.push((g * 255.0) as u8);
            texture_data.push((b * 255.0) as u8);
            texture_data.push((a * 255.0) as u8);
        }

        texture_data
    }
}

/// Neural visualization modes
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum NeuralVisualizationMode {
    /// Heatmap of activations
    Heatmap,
    /// Attention flow visualization
    AttentionFlow,
    /// Memory pattern visualization
    MemoryPattern,
    /// Combined multi-channel view
    MultiChannel,
}

/// Neural state renderer configuration
#[derive(Debug, Clone)]
pub struct NeuralRendererConfig {
    pub mode: NeuralVisualizationMode,
    pub color_scale: f32,
    pub brightness: f32,
    pub contrast: f32,
    pub show_grid: bool,
    pub grid_size: u32,
}

impl Default for NeuralRendererConfig {
    fn default() -> Self {
        Self {
            mode: NeuralVisualizationMode::MultiChannel,
            color_scale: 1.0,
            brightness: 1.0,
            contrast: 1.0,
            show_grid: true,
            grid_size: 16,
        }
    }
}

/// Helper functions for neural visualization
impl NeuralStateTexture {
    /// Create a heatmap visualization
    pub fn create_heatmap(data: &[f32], width: u32, height: u32) -> Vec<[f32; 4]> {
        let mut heatmap = Vec::with_capacity((width * height) as usize);

        for i in 0..data.len() {
            let value = data[i].clamp(0.0, 1.0);
            
            // Color gradient: blue -> cyan -> green -> yellow -> red
            let color = if value < 0.25 {
                // Blue to cyan
                let t = value / 0.25;
                [0.0, t, 1.0, 1.0]
            } else if value < 0.5 {
                // Cyan to green
                let t = (value - 0.25) / 0.25;
                [0.0, 1.0, 1.0 - t, 1.0]
            } else if value < 0.75 {
                // Green to yellow
                let t = (value - 0.5) / 0.25;
                [t, 1.0, 0.0, 1.0]
            } else {
                // Yellow to red
                let t = (value - 0.75) / 0.25;
                [1.0, 1.0 - t, 0.0, 1.0]
            };

            heatmap.push(color);
        }

        heatmap
    }

    /// Create attention flow visualization
    pub fn create_attention_flow(
        attention: &[f32],
        width: u32,
        height: u32,
    ) -> Vec<[f32; 4]> {
        let mut flow = Vec::with_capacity((width * height) as usize);

        for i in 0..attention.len() {
            let value = attention[i].clamp(0.0, 1.0);
            
            // Flow visualization: intensity-based with direction hints
            let intensity = value * 2.0; // Amplify for visibility
            let color = [
                intensity.min(1.0),
                intensity.min(1.0) * 0.8,
                intensity.min(1.0) * 0.6,
                value,
            ];

            flow.push(color);
        }

        flow
    }

    /// Create memory pattern visualization
    pub fn create_memory_pattern(
        memory: &[f32],
        width: u32,
        height: u32,
    ) -> Vec<[f32; 4]> {
        let mut pattern = Vec::with_capacity((width * height) as usize);

        for i in 0..memory.len() {
            let value = memory[i].clamp(-1.0, 1.0);
            
            // Memory pattern: positive = warm colors, negative = cool colors
            let (r, g, b) = if value >= 0.0 {
                // Positive: orange/yellow
                (1.0, 0.7 * value, 0.0)
            } else {
                // Negative: blue/cyan
                (0.0, 0.7 * (-value), 1.0)
            };

            pattern.push([r, g, b, value.abs()]);
        }

        pattern
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_neural_state_texture_creation() {
        // This test would require a wgpu::Device, which is complex to set up
        // For now, we'll just verify the data conversion logic
        let neural_state = NeuralStateData {
            layer_activations: vec![0.5, 0.7, 0.3],
            attention_weights: vec![0.8, 0.2, 0.9],
            memory_patterns: vec![0.4, 0.6, 0.1],
            confidence: 0.85,
            cognitive_load: 0.2,
            attention_focus: [0.0, 0.0],
            emotional_valence: 0.0,
            arousal: 0.5,
            thought_stream: vec![],
            active_regions: vec![],
            thought_vector: vec![0.1, 0.2, 0.3],
            self_state: SelfState::default(),
            theory_of_mind: TheoryOfMindState::default(),
            cognitive_state: CognitiveState::default(),
            audio_features: AudioFeatures::default(),
            input_state: InputState::default(),
            token_visualization: None,
        };

        // Verify data structure
        assert_eq!(neural_state.layer_activations.len(), 3);
        assert_eq!(neural_state.confidence, 0.85);
    }

    #[test]
    fn test_heatmap_creation() {
        let data = vec![0.0, 0.25, 0.5, 0.75, 1.0];
        let heatmap = NeuralStateTexture::create_heatmap(&data, 5, 1);

        assert_eq!(heatmap.len(), 5);
        
        // Check gradient progression
        assert!(heatmap[0][2] > 0.9); // Blue
        assert!(heatmap[2][1] > 0.9); // Green
        assert!(heatmap[4][0] > 0.9); // Red
    }

    #[test]
    fn test_attention_flow_creation() {
        let attention = vec![0.0, 0.5, 1.0];
        let flow = NeuralStateTexture::create_attention_flow(&attention, 3, 1);

        assert_eq!(flow.len(), 3);
        assert!(flow[0][3] == 0.0); // Low attention
        assert!(flow[2][3] == 1.0); // High attention
    }

    #[test]
    fn test_memory_pattern_creation() {
        let memory = vec![-1.0, 0.0, 1.0];
        let pattern = NeuralStateTexture::create_memory_pattern(&memory, 3, 1);

        assert_eq!(pattern.len(), 3);
        assert!(pattern[0][2] > 0.9); // Negative = blue
        assert!(pattern[2][0] > 0.9); // Positive = orange
    }
}
