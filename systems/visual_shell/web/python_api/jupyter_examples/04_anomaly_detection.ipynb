{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection with Neural Heatmap API\n",
    "\n",
    "This notebook demonstrates anomaly detection and analysis capabilities.\n",
    "\n",
    "## What You'll Learn\n",
    "- Retrieve detected anomalies\n",
    "- Filter anomalies by severity and type\n",
    "- Visualize anomaly patterns\n",
    "- Export anomaly data for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from neural_heatmap import NeuralHeatmapClient, connect, Anomaly\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def connect_to_server():\n",
    "    client = await connect(\"http://localhost:8080\")\n",
    "    if client.connected:\n",
    "        print(\"✅ Connected to Neural Heatmap server\")\n",
    "    return client\n",
    "\n",
    "client = await connect_to_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get All Anomalies\n",
    "\n",
    "Retrieve all detected anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_all_anomalies():\n",
    "    # Get all anomalies\n",
    "    anomalies = await client.get_anomalies()\n",
    "    \n",
    "    print(f\"Found {len(anomalies)} anomalies\")\n",
    "    \n",
    "    # Group by severity\n",
    "    severity_counts = {}\n",
    "    for anomaly in anomalies:\n",
    "        severity = anomaly.severity\n",
    "        if severity not in severity_counts:\n",
    "            severity_counts[severity] = 0\n",
    "        severity_counts[severity] += 1\n",
    "    \n",
    "    print(\"\\nSeverity Distribution:\")\n",
    "    for severity, count in sorted(severity_counts.items()):\n",
    "        print(f\"  {severity.title()}: {count}\")\n",
    "    \n",
    "    return anomalies, severity_counts\n",
    "\n",
    "anomalies, severity_counts = await get_all_anomalies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Severity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_severity_distribution(severity_counts):\n",
    "    \"\"\"Plot severity distribution\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    severities = list(severity_counts.keys())\n",
    "    counts = list(severity_counts.values())\n",
    "    \n",
    "    # Color mapping for severity\n",
    "    color_map = {\n",
    "        'low': '#2ca02c',\n",
    "        'medium': '#ff7f0e',\n",
    "        'high': '#d62728',\n",
    "        'critical': '#9467bd'\n",
    "    }\n",
    "    colors = [color_map.get(s, '#1f77b4') for s in severities]\n",
    "    \n",
    "    bars = ax.bar(severities, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Severity', fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title('Anomaly Severity Distribution', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{int(height)}',\n",
    "               ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_severity_distribution(severity_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter by Severity\n",
    "\n",
    "Get anomalies of specific severity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_high_severity_anomalies():\n",
    "    # Get high and critical severity anomalies\n",
    "    high_anomalies = await client.get_anomalies(severity=\"high\")\n",
    "    critical_anomalies = await client.get_anomalies(severity=\"critical\")\n",
    "    \n",
    "    print(f\"High severity anomalies: {len(high_anomalies)}\")\n",
    "    print(f\"Critical severity anomalies: {len(critical_anomalies)}\")\n",
    "    \n",
    "    # Display details\n",
    "    if high_anomalies:\n",
    "        print(\"\\nHigh Severity Anomalies:\")\n",
    "        for anomaly in high_anomalies[:5]:\n",
    "            print(f\"  - {anomaly.description[:80]}...\")\n",
    "            print(f\"    Score: {anomaly.score:.3f}, Layer: {anomaly.layer}\")\n",
    "    \n",
    "    if critical_anomalies:\n",
    "        print(\"\\nCritical Severity Anomalies:\")\n",
    "        for anomaly in critical_anomalies[:5]:\n",
    "            print(f\"  - {anomaly.description[:80]}...\")\n",
    "            print(f\"    Score: {anomaly.score:.3f}, Layer: {anomaly.layer}\")\n",
    "    \n",
    "    return high_anomalies, critical_anomalies\n",
    "\n",
    "high_anomalies, critical_anomalies = await get_high_severity_anomalies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Anomaly Types\n",
    "\n",
    "Group anomalies by type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_anomaly_types(anomalies):\n",
    "    \"\"\"Group and analyze anomalies by type\"\"\"\n",
    "    type_counts = {}\n",
    "    type_details = {}\n",
    "    \n",
    "    for anomaly in anomalies:\n",
    "        atype = anomaly.anomaly_type\n",
    "        if atype not in type_counts:\n",
    "            type_counts[atype] = 0\n",
    "            type_details[atype] = []\n",
    "        type_counts[atype] += 1\n",
    "        type_details[atype].append(anomaly)\n",
    "    \n",
    "    print(\"\\nAnomaly Types:\")\n",
    "    for atype, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {atype}: {count}\")\n",
    "    \n",
    "    return type_counts, type_details\n",
    "\n",
    "type_counts, type_details = analyze_anomaly_types(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Anomaly Timeline\n",
    "\n",
    "Create a timeline visualization of anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomaly_timeline(anomalies):\n",
    "    \"\"\"Create timeline visualization of anomalies\"\"\"\n",
    "    if not anomalies:\n",
    "        print(\"No anomalies to visualize\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    sorted_anomalies = sorted(anomalies, key=lambda a: a.timestamp)\n",
    "    \n",
    "    # Color mapping\n",
    "    color_map = {\n",
    "        'low': '#2ca02c',\n",
    "        'medium': '#ff7f0e',\n",
    "        'high': '#d62728',\n",
    "        'critical': '#9467bd'\n",
    "    }\n",
    "    \n",
    "    # Plot anomalies\n",
    "    timestamps = [a.timestamp for a in sorted_anomalies]\n",
    "    scores = [a.score for a in sorted_anomalies]\n",
    "    colors = [color_map.get(a.severity, '#1f77b4') for a in sorted_anomalies]\n",
    "    sizes = [100 + a.score * 100 for a in sorted_anomalies]\n",
    "    \n",
    "    scatter = ax.scatter(timestamps, scores, c=colors, s=sizes, alpha=0.6, edgecolors='black')\n",
    "    \n",
    "    ax.set_xlabel('Time', fontsize=12)\n",
    "    ax.set_ylabel('Anomaly Score', fontsize=12)\n",
    "    ax.set_title('Anomaly Timeline', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Create legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=color_map[s], label=s.title()) \n",
    "                      for s in color_map.keys()]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_anomaly_timeline(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spatial Distribution of Anomalies\n",
    "\n",
    "Visualize anomalies in spatial context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spatial_distribution(anomalies):\n",
    "    \"\"\"Plot spatial distribution of anomalies\"\"\"\n",
    "    # Filter anomalies with location data\n",
    "    located_anomalies = [a for a in anomalies if a.location]\n",
    "    \n",
    "    if not located_anomalies:\n",
    "        print(\"No anomalies with location data\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Color by severity\n",
    "    color_map = {\n",
    "        'low': '#2ca02c',\n",
    "        'medium': '#ff7f0e',\n",
    "        'high': '#d62728',\n",
    "        'critical': '#9467bd'\n",
    "    }\n",
    "    \n",
    "    # Group by severity\n",
    "    for severity in color_map.keys():\n",
    "        severity_anomalies = [a for a in located_anomalies if a.severity == severity]\n",
    "        \n",
    "        if severity_anomalies:\n",
    "            x_coords = [a.location.get('x', 0) for a in severity_anomalies]\n",
    "            y_coords = [a.location.get('y', 0) for a in severity_anomalies]\n",
    "            scores = [a.score for a in severity_anomalies]\n",
    "            \n",
    "            scatter = ax.scatter(x_coords, y_coords, \n",
    "                               c=[color_map[severity]], \n",
    "                               s=[100 + s * 200 for s in scores],\n",
    "                               alpha=0.6,\n",
    "                               edgecolors='black',\n",
    "                               label=severity.title())\n",
    "    \n",
    "    ax.set_xlabel('X Coordinate', fontsize=12)\n",
    "    ax.set_ylabel('Y Coordinate', fontsize=12)\n",
    "    ax.set_title('Spatial Distribution of Anomalies', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_spatial_distribution(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Layer-wise Anomaly Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layer_anomalies(anomalies):\n",
    "    \"\"\"Analyze anomalies by neural layer\"\"\"\n",
    "    layer_anomalies = {}\n",
    "    \n",
    "    for anomaly in anomalies:\n",
    "        layer = anomaly.layer or \"unknown\"\n",
    "        if layer not in layer_anomalies:\n",
    "            layer_anomalies[layer] = []\n",
    "        layer_anomalies[layer].append(anomaly)\n",
    "    \n",
    "    print(\"\\nAnomalies by Layer:\")\n",
    "    print(f\"{'Layer':<30} {'Count':<10} {'Avg Score':<12} {'Max Severity':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for layer, layer_list in sorted(layer_anomalies.items(), \n",
    "                                    key=lambda x: len(x[1]), \n",
    "                                    reverse=True):\n",
    "        count = len(layer_list)\n",
    "        avg_score = sum(a.score for a in layer_list) / count\n",
    "        max_severity = max(a.severity for a in layer_list)\n",
    "        print(f\"{layer:<30} {count:<10} {avg_score:<12.3f} {max_severity:<15}\")\n",
    "    \n",
    "    return layer_anomalies\n",
    "\n",
    "layer_anomalies = analyze_layer_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Anomaly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def export_anomaly_data():\n",
    "    \"\"\"Export anomaly data\"\"\"\n",
    "    export_dir = Path(\"./anomaly_analysis\")\n",
    "    export_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Export to CSV\n",
    "    await client.download_anomalies(\n",
    "        export_dir / \"anomalies.csv\"\n",
    "    )\n",
    "    print(\"✅ Exported anomalies.csv\")\n",
    "    \n",
    "    # Export high severity to JSON\n",
    "    import json\n",
    "    \n",
    "    high_severity_data = [\n",
    "        {\n",
    "            \"type\": a.anomaly_type,\n",
    "            \"severity\": a.severity,\n",
    "            \"score\": a.score,\n",
    "            \"description\": a.description,\n",
    "            \"layer\": a.layer,\n",
    "            \"location\": a.location,\n",
    "            \"timestamp\": a.timestamp,\n",
    "            \"metadata\": a.metadata\n",
    "        }\n",
    "        for a in high_anomalies + critical_anomalies\n",
    "    ]\n",
    "    \n",
    "    with open(export_dir / \"high_severity_anomalies.json\", \"w\") as f:\n",
    "        json.dump(high_severity_data, f, indent=2)\n",
    "    print(\"✅ Exported high_severity_anomalies.json\")\n",
    "    \n",
    "    # Export summary statistics\n",
    "    summary = {\n",
    "        \"total_anomalies\": len(anomalies),\n",
    "        \"severity_distribution\": severity_counts,\n",
    "        \"type_distribution\": type_counts,\n",
    "        \"high_severity_count\": len(high_anomalies),\n",
    "        \"critical_count\": len(critical_anomalies),\n",
    "        \"layer_counts\": {layer: len(anoms) for layer, anoms in layer_anomalies.items()}\n",
    "    }\n",
    "    \n",
    "    with open(export_dir / \"anomaly_summary.json\", \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(\"✅ Exported anomaly_summary.json\")\n",
    "\n",
    "await export_anomaly_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Anomaly Alert System\n",
    "\n",
    "Set up real-time monitoring for critical anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def monitor_critical_anomalies():\n",
    "    \"\"\"Subscribe to real-time anomaly updates\"\"\"\n",
    "    # Subscribe to anomaly updates\n",
    "    await client.subscribe_anomalies()\n",
    "    \n",
    "    print(\"Monitoring for critical anomalies...\")\n",
    "    print(\"(Press Ctrl+C to stop)\")\n",
    "    \n",
    "    try:\n",
    "        async for update in client.stream_updates(message_types=[\"anomaly_update\"]):\n",
    "            if update.severity in [\"high\", \"critical\"]:\n",
    "                timestamp = datetime.fromtimestamp(update.timestamp / 1000)\n",
    "                print(f\"\\n⚠️ [{timestamp}] {update.severity.upper()} ANOMALY DETECTED\")\n",
    "                print(f\"   Description: {update.description}\")\n",
    "                print(f\"   Score: {update.score:.3f}\")\n",
    "                if update.location:\n",
    "                    print(f\"   Location: ({update.location.get('x', 0)}, {update.location.get('y', 0)})\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopping anomaly monitor...\")\n",
    "\n",
    "# Uncomment to run the monitor:\n",
    "# await monitor_critical_anomalies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def cleanup():\n",
    "    await client.unsubscribe_anomalies()\n",
    "    await client.disconnect()\n",
    "    print(\"✅ Disconnected from server\")\n",
    "\n",
    "await cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "- How to retrieve and filter anomalies\n",
    "- Analyzing anomaly severity and types\n",
    "- Visualizing anomaly timelines and spatial distributions\n",
    "- Layer-wise anomaly analysis\n",
    "- Exporting anomaly data\n",
    "- Setting up real-time anomaly monitoring\n",
    "\n",
    "## Next Steps\n",
    "- `05_custom_workflows.ipynb` - Build custom analysis workflows\n",
    "- Combine anomaly detection with temporal analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
