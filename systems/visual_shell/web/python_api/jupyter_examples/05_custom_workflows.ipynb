{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Workflows with Neural Heatmap API\n",
    "\n",
    "This notebook demonstrates how to combine multiple API operations into custom workflows.\n",
    "\n",
    "## What You'll Learn\n",
    "- Combine multiple API calls into workflows\n",
    "- Create custom analysis pipelines\n",
    "- Automate data collection and export\n",
    "- Build real-time monitoring systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from neural_heatmap import (\n",
    "    NeuralHeatmapClient, \n",
    "    connect, \n",
    "    FilterConfig, \n",
    "    VisualizationTheme,\n",
    "    ExportFormat\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Workflow: Comprehensive Analysis Pipeline\n",
    "\n",
    "Combine correlation, temporal, and anomaly analysis into a single workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def comprehensive_analysis_workflow(client, output_dir=\"./analysis_results\"):\n",
    "    \"\"\"\n",
    "    Run a comprehensive analysis combining multiple API operations.\n",
    "    \n",
    "    This workflow:\n",
    "    1. Gets correlation matrix\n",
    "    2. Analyzes temporal patterns\n",
    "    3. Detects anomalies\n",
    "    4. Exports all results\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"üîç Starting Comprehensive Analysis Workflow\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Get correlation matrix\n",
    "    print(\"\\n1Ô∏è‚É£ Fetching correlation matrix...\")\n",
    "    correlation_df = await client.get_correlation_matrix(as_dataframe=True)\n",
    "    print(f\"   ‚úì Retrieved {correlation_df.shape[0]}x{correlation_df.shape[1]} matrix\")\n",
    "    \n",
    "    # 2. Get temporal patterns\n",
    "    print(\"\\n2Ô∏è‚É£ Analyzing temporal patterns...\")\n",
    "    temporal_patterns = await client.get_temporal_patterns()\n",
    "    print(f\"   ‚úì Found {len(temporal_patterns)} patterns\")\n",
    "    \n",
    "    # 3. Get anomalies\n",
    "    print(\"\\n3Ô∏è‚É£ Detecting anomalies...\")\n",
    "    anomalies = await client.get_anomalies()\n",
    "    print(f\"   ‚úì Found {len(anomalies)} anomalies\")\n",
    "    \n",
    "    # 4. Get temporal statistics\n",
    "    print(\"\\n4Ô∏è‚É£ Gathering temporal statistics...\")\n",
    "    temporal_stats = await client.get_temporal_statistics()\n",
    "    print(f\"   ‚úì Statistics collected\")\n",
    "    \n",
    "    # 5. Export all data\n",
    "    print(\"\\n5Ô∏è‚É£ Exporting results...\")\n",
    "    correlation_df.to_csv(output_path / \"correlation_matrix.csv\")\n",
    "    print(f\"   ‚úì Exported correlation matrix\")\n",
    "    \n",
    "    # Export patterns and anomalies as JSON\n",
    "    results = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"correlation\": {\n",
    "            \"shape\": correlation_df.shape,\n",
    "            \"layers\": list(correlation_df.columns)\n",
    "        },\n",
    "        \"temporal_patterns\": [\n",
    "            {\n",
    "                \"type\": p.pattern_type,\n",
    "                \"confidence\": p.confidence,\n",
    "                \"frequency\": p.frequency,\n",
    "                \"metadata\": p.metadata\n",
    "            }\n",
    "            for p in temporal_patterns\n",
    "        ],\n",
    "        \"anomalies\": [\n",
    "            {\n",
    "                \"type\": a.anomaly_type,\n",
    "                \"severity\": a.severity,\n",
    "                \"score\": a.score,\n",
    "                \"description\": a.description\n",
    "            }\n",
    "            for a in anomalies\n",
    "        ],\n",
    "        \"temporal_stats\": temporal_stats\n",
    "    }\n",
    "    \n",
    "    with open(output_path / \"comprehensive_analysis.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"   ‚úì Exported analysis results\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"‚úÖ Comprehensive Analysis Complete!\")\n",
    "    print(f\"   Results saved to: {output_path.absolute()}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Connect and run workflow\n",
    "client = await connect(\"http://localhost:8080\")\n",
    "results = await comprehensive_analysis_workflow(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Workflow: Filtered Multi-Model Comparison\n",
    "\n",
    "Compare correlations across multiple models with custom filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def multi_model_comparison_workflow(client, model_ids, layer_filter=None):\n",
    "    \"\"\"\n",
    "    Compare correlations across multiple models with optional layer filtering.\n",
    "    \"\"\"\n",
    "    print(f\"üî¨ Multi-Model Comparison Workflow\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Models: {model_ids}\")\n",
    "    if layer_filter:\n",
    "        print(f\"Layer Filter: {layer_filter}\")\n",
    "    print()\n",
    "    \n",
    "    comparison_results = {}\n",
    "    \n",
    "    for model_id in model_ids:\n",
    "        print(f\"\\nüìä Analyzing model: {model_id}\")\n",
    "        \n",
    "        # Apply filter if specified\n",
    "        if layer_filter:\n",
    "            await client.set_filter(\n",
    "                model_ids=[model_id],\n",
    "                layer_ids=layer_filter\n",
    "            )\n",
    "        else:\n",
    "            await client.set_filter(model_ids=[model_id])\n",
    "        \n",
    "        # Get correlation matrix\n",
    "        try:\n",
    "            corr_df = await client.get_correlation_matrix(as_dataframe=True)\n",
    "            comparison_results[model_id] = {\n",
    "                \"correlation_matrix\": corr_df,\n",
    "                \"mean_correlation\": corr_df.values[np.triu_indices_from(corr_df.values, k=1)].mean(),\n",
    "                \"max_correlation\": corr_df.values[np.triu_indices_from(corr_df.values, k=1)].max(),\n",
    "                \"min_correlation\": corr_df.values[np.triu_indices_from(corr_df.values, k=1)].min()\n",
    "            }\n",
    "            print(f\"  Mean Correlation: {comparison_results[model_id]['mean_correlation']:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            comparison_results[model_id] = None\n",
    "    \n",
    "    # Clear filter\n",
    "    await client.clear_filter()\n",
    "    \n",
    "    # Create comparison summary\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üìà Comparison Summary\")\n",
    "    print(f\"{'Model':<20} {'Mean':<10} {'Max':<10} {'Min':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_id, results in comparison_results.items():\n",
    "        if results:\n",
    "            print(f\"{model_id:<20} {results['mean_correlation']:<10.3f} \"\n",
    "                  f\"{results['max_correlation']:<10.3f} {results['min_correlation']:<10.3f}\")\n",
    "    \n",
    "    return comparison_results\n",
    "\n",
    "# Example: Compare multiple models\n",
    "# comparison = await multi_model_comparison_workflow(\n",
    "#     client,\n",
    "#     model_ids=[\"model1\", \"model2\", \"model3\"],\n",
    "#     layer_filter=[\"layer1\", \"layer2\", \"layer3\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Workflow: Real-Time Monitoring Dashboard\n",
    "\n",
    "Set up real-time monitoring with custom alerting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyMonitor:\n",
    "    \"\"\"\n",
    "    Real-time anomaly monitoring system with custom alerting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client, alert_threshold=0.8):\n",
    "        self.client = client\n",
    "        self.alert_threshold = alert_threshold\n",
    "        self.alert_count = 0\n",
    "        self.start_time = None\n",
    "    \n",
    "    async def start(self):\n",
    "        \"\"\"Start monitoring\"\"\"\n",
    "        await self.client.subscribe_anomalies()\n",
    "        self.start_time = datetime.now()\n",
    "        print(f\"üö® Anomaly Monitor Started\")\n",
    "        print(f\"   Alert Threshold: {self.alert_threshold}\")\n",
    "        print(f\"   Started at: {self.start_time.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    async def stop(self):\n",
    "        \"\"\"Stop monitoring\"\"\"\n",
    "        await self.client.unsubscribe_anomalies()\n",
    "        elapsed = (datetime.now() - self.start_time).total_seconds()\n",
    "        print(f\"\\nüõë Monitoring Stopped\")\n",
    "        print(f\"   Duration: {elapsed:.1f} seconds\")\n",
    "        print(f\"   Alerts Triggered: {self.alert_count}\")\n",
    "    \n",
    "    async def monitor(self, duration=60):\n",
    "        \"\"\"\n",
    "        Monitor for specified duration (in seconds).\n",
    "        Returns summary of alerts.\n",
    "        \"\"\"\n",
    "        await self.start()\n",
    "        \n",
    "        alerts = []\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        try:\n",
    "            async for update in self.client.stream_updates(message_types=[\"anomaly_update\"]):\n",
    "                # Check if alert threshold exceeded\n",
    "                if update.score >= self.alert_threshold:\n",
    "                    alert = {\n",
    "                        \"timestamp\": update.timestamp,\n",
    "                        \"severity\": update.severity,\n",
    "                        \"score\": update.score,\n",
    "                        \"description\": update.description,\n",
    "                        \"layer\": update.layer\n",
    "                    }\n",
    "                    alerts.append(alert)\n",
    "                    self.alert_count += 1\n",
    "                    \n",
    "                    # Print alert\n",
    "                    print(f\"\\n‚ö†Ô∏è ALERT #{self.alert_count}\")\n",
    "                    print(f\"   Severity: {alert['severity'].upper()}\")\n",
    "                    print(f\"   Score: {alert['score']:.3f}\")\n",
    "                    print(f\"   Description: {alert['description']}\")\n",
    "                \n",
    "                # Check if duration exceeded\n",
    "                elapsed = (datetime.now() - start_time).total_seconds()\n",
    "                if elapsed >= duration:\n",
    "                    break\n",
    "                    \n",
    "        except asyncio.CancelledError:\n",
    "            pass\n",
    "        \n",
    "        await self.stop()\n",
    "        \n",
    "        return {\n",
    "            \"duration\": duration,\n",
    "            \"total_alerts\": len(alerts),\n",
    "            \"alerts\": alerts\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "# monitor = AnomalyMonitor(client, alert_threshold=0.7)\n",
    "# summary = await monitor.monitor(duration=30)\n",
    "# print(f\"\\nMonitoring Summary: {summary['total_alerts']} alerts in {summary['duration']}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Workflow: Automated Data Collection\n",
    "\n",
    "Automatically collect and archive data at regular intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def automated_collection_workflow(client, interval_seconds=60, num_collections=5):\n",
    "    \"\"\"\n",
    "    Automatically collect data at regular intervals.\n",
    "    \n",
    "    Args:\n",
    "        client: NeuralHeatmapClient instance\n",
    "        interval_seconds: Time between collections\n",
    "        num_collections: Number of collections to perform\n",
    "    \"\"\"\n",
    "    archive_dir = Path(\"./data_archive\")\n",
    "    archive_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"üì¶ Automated Data Collection\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Interval: {interval_seconds}s\")\n",
    "    print(f\"Collections: {num_collections}\")\n",
    "    print(f\"Archive: {archive_dir.absolute()}\")\n",
    "    print()\n",
    "    \n",
    "    collection_summary = []\n",
    "    \n",
    "    for i in range(num_collections):\n",
    "        timestamp = datetime.now()\n",
    "        timestamp_str = timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        collection_dir = archive_dir / f\"collection_{timestamp_str}\"\n",
    "        collection_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nüì• Collection {i+1}/{num_collections} [{timestamp.strftime('%H:%M:%S')}]\")\n",
    "        \n",
    "        try:\n",
    "            # Collect correlation matrix\n",
    "            corr_df = await client.get_correlation_matrix(as_dataframe=True)\n",
    "            corr_df.to_csv(collection_dir / \"correlation.csv\")\n",
    "            print(f\"  ‚úì Correlation matrix\")\n",
    "            \n",
    "            # Collect temporal patterns\n",
    "            patterns = await client.get_temporal_patterns()\n",
    "            with open(collection_dir / \"temporal_patterns.json\", \"w\") as f:\n",
    "                json.dump([{\n",
    "                    \"type\": p.pattern_type,\n",
    "                    \"confidence\": p.confidence,\n",
    "                    \"frequency\": p.frequency,\n",
    "                    \"metadata\": p.metadata\n",
    "                } for p in patterns], f)\n",
    "            print(f\"  ‚úì Temporal patterns ({len(patterns)} found)\")\n",
    "            \n",
    "            # Collect anomalies\n",
    "            anomalies = await client.get_anomalies()\n",
    "            with open(collection_dir / \"anomalies.json\", \"w\") as f:\n",
    "                json.dump([{\n",
    "                    \"type\": a.anomaly_type,\n",
    "                    \"severity\": a.severity,\n",
    "                    \"score\": a.score,\n",
    "                    \"description\": a.description\n",
    "                } for a in anomalies], f)\n",
    "            print(f\"  ‚úì Anomalies ({len(anomalies)} found)\")\n",
    "            \n",
    "            # Record summary\n",
    "            collection_summary.append({\n",
    "                \"timestamp\": timestamp.isoformat(),\n",
    "                \"directory\": str(collection_dir),\n",
    "                \"patterns\": len(patterns),\n",
    "                \"anomalies\": len(anomalies)\n",
    "            })\n",
    "            \n",
    "            print(f\"  ‚úì Saved to {collection_dir.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error: {e}\")\n",
    "            collection_summary.append({\n",
    "                \"timestamp\": timestamp.isoformat(),\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "        \n",
    "        # Wait for next interval (unless it's the last collection)\n",
    "        if i < num_collections - 1:\n",
    "            print(f\"  ‚è≥ Waiting {interval_seconds}s...\")\n",
    "            await asyncio.sleep(interval_seconds)\n",
    "    \n",
    "    # Save collection summary\n",
    "    with open(archive_dir / \"collection_summary.json\", \"w\") as f:\n",
    "        json.dump(collection_summary, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"‚úÖ Data Collection Complete\")\n",
    "    print(f\"   Collections: {len(collection_summary)}\")\n",
    "    print(f\"   Archive: {archive_dir.absolute()}\")\n",
    "    \n",
    "    return collection_summary\n",
    "\n",
    "# Example usage:\n",
    "# summary = await automated_collection_workflow(client, interval_seconds=30, num_collections=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Workflow: Custom Theme and Layout\n",
    "\n",
    "Apply custom visualization themes and layouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def customization_workflow(client):\n",
    "    \"\"\"\n",
    "    Apply custom themes and layouts to the visualization.\n",
    "    \"\"\"\n",
    "    print(\"üé® Customization Workflow\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Apply themes\n",
    "    themes = [\n",
    "        VisualizationTheme.THERMAL,\n",
    "        VisualizationTheme.PLASMA,\n",
    "        VisualizationTheme.HOLOGRAPHIC,\n",
    "        VisualizationTheme.CONTOUR\n",
    "    ]\n",
    "    \n",
    "    for theme in themes:\n",
    "        print(f\"\\nüé≠ Applying theme: {theme.value}\")\n",
    "        success = await client.set_theme(theme)\n",
    "        if success:\n",
    "            print(f\"   ‚úì Theme applied\")\n",
    "        else:\n",
    "            print(f\"   ‚úó Failed to apply theme\")\n",
    "        await asyncio.sleep(1)  # Brief pause to see the change\n",
    "    \n",
    "    # Apply custom layout\n",
    "    print(\"\\nüìê Applying custom layout...\")\n",
    "    \n",
    "    custom_layout = {\n",
    "        \"panel_positions\": {\n",
    "            \"heatmap\": {\"x\": 0, \"y\": 0},\n",
    "            \"correlation\": {\"x\": 400, \"y\": 0},\n",
    "            \"temporal\": {\"x\": 0, \"y\": 300},\n",
    "            \"anomalies\": {\"x\": 400, \"y\": 300}\n",
    "        },\n",
    "        \"panel_sizes\": {\n",
    "            \"heatmap\": {\"width\": 350, \"height\": 250},\n",
    "            \"correlation\": {\"width\": 350, \"height\": 250},\n",
    "            \"temporal\": {\"width\": 350, \"height\": 200},\n",
    "            \"anomalies\": {\"width\": 350, \"height\": 200}\n",
    "        },\n",
    "        \"visibility\": {\n",
    "            \"heatmap\": True,\n",
    "            \"correlation\": True,\n",
    "            \"temporal\": True,\n",
    "            \"anomalies\": True\n",
    "        },\n",
    "        \"z_order\": [\"heatmap\", \"correlation\", \"temporal\", \"anomalies\"]\n",
    "    }\n",
    "    \n",
    "    success = await client.set_layout(custom_layout)\n",
    "    if success:\n",
    "        print(\"   ‚úì Custom layout applied\")\n",
    "    else:\n",
    "        print(\"   ‚úó Failed to apply layout\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"‚úÖ Customization Complete\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# await customization_workflow(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Workflow: Export for TensorBoard\n",
    "\n",
    "Prepare data for visualization in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def tensorboard_export_workflow(client, output_dir=\"./tensorboard_logs\"):\n",
    "    \"\"\"\n",
    "    Export data in TensorBoard format.\n",
    "    \"\"\"\n",
    "    print(\"üìä TensorBoard Export Workflow\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    log_dir = await client.export_tensorboard(\n",
    "        output_dir=output_dir,\n",
    "        run_name=f\"neural_heatmap_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ TensorBoard logs exported to:\")\n",
    "    print(f\"   {log_dir}\")\n",
    "    print(f\"\\nTo view in TensorBoard:\")\n",
    "    print(f\"   tensorboard --logdir={Path(log_dir).parent}\")\n",
    "    \n",
    "    return log_dir\n",
    "\n",
    "# Uncomment to run:\n",
    "# log_dir = await tensorboard_export_workflow(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Workflow: Batch Export\n",
    "\n",
    "Export all available data in multiple formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def batch_export_workflow(client, output_dir=\"./batch_export\"):\n",
    "    \"\"\"\n",
    "    Export all data in multiple formats.\n",
    "    \"\"\"\n",
    "    print(\"üì¶ Batch Export Workflow\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    formats = [ExportFormat.CSV, ExportFormat.JSON]\n",
    "    \n",
    "    exported = await client.export_all(\n",
    "        output_dir=output_dir,\n",
    "        formats=formats\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Exported {len(exported)} files:\")\n",
    "    for data_type, filepath in exported.items():\n",
    "        print(f\"   {data_type}: {filepath}\")\n",
    "    \n",
    "    return exported\n",
    "\n",
    "# Uncomment to run:\n",
    "# exported = await batch_export_workflow(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def cleanup():\n",
    "    await client.disconnect()\n",
    "    print(\"‚úÖ Disconnected from server\")\n",
    "\n",
    "await cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "- How to combine multiple API operations into workflows\n",
    "- Creating automated data collection pipelines\n",
    "- Building real-time monitoring systems\n",
    "- Applying custom themes and layouts\n",
    "- Exporting data for external tools like TensorBoard\n",
    "\n",
    "## Next Steps\n",
    "- Create your own custom workflows\n",
    "- Integrate with your existing analysis pipelines\n",
    "- Build automated alerting systems\n",
    "- Combine with machine learning models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
