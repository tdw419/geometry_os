name: Performance Benchmarks

on:
  # Trigger on push to main branches
  push:
    branches:
      - main
      - master
    paths:
      - 'geometry_os/**'
      - '.github/workflows/performance-benchmarks.yml'

  # Trigger on pull requests
  pull_request:
    branches:
      - main
      - master
    paths:
      - 'geometry_os/**'
      - '.github/workflows/performance-benchmarks.yml'

  # Trigger weekly on Sunday at 03:00 UTC
  schedule:
    - cron: '0 3 * * 0'

  # Allow manual workflow dispatch
  workflow_dispatch:
    inputs:
      benchmark_suite:
        description: 'Benchmark suite to run'
        required: false
        type: choice
        options:
          - all
          - rust
          - python
          - neural
          - memory
        default: 'all'
      iterations:
        description: 'Number of benchmark iterations'
        required: false
        type: number
        default: 10

jobs:
  # ============================================================================
  # Rust Performance Benchmarks
  # ============================================================================
  rust-benchmarks:
    name: Rust Performance Benchmarks
    runs-on: ubuntu-latest
    if: |
      github.event.inputs.benchmark_suite == 'all' ||
      github.event.inputs.benchmark_suite == 'rust' ||
      github.event.inputs.benchmark_suite == ''

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: stable

      - name: Cache Rust dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            geometry_os/systems/infinite_map_rs/target
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-bench-

      - name: Install benchmark dependencies
        run: |
          cargo install critcmp
          # Install criterion for benchmarking
          cd geometry_os/systems/infinite_map_rs
          cargo install cargo-criterion

      - name: Run Rust benchmarks
        id: rust_bench
        run: |
          cd geometry_os/systems/infinite_map_rs

          # Run benchmarks with criterion
          cargo criterion --message-format=json > /tmp/rust_bench_results.json 2>&1 || true

          # Also run basic benchmarks if criterion not configured
          if [ ! -f "benches/benchmark.rs" ]; then
            echo "No Rust benchmarks configured, running basic performance tests..."
            cargo test --release -- --nocapture --test-threads=1 || true
          fi

      - name: Generate Rust benchmark report
        if: always()
        run: |
          mkdir -p reports/rust

          cat > reports/rust/benchmark-summary.md <<EOF
          # Rust Performance Benchmarks

          **Workflow Run:** #${{ github.run_number }}
          **Event:** ${{ github.event_name }}
          **Branch:** ${{ github.ref_name }}
          **Commit:** ${{ github.sha }}
          **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")

          ## Benchmark Results

          EOF

          # Parse JSON results if available
          if [ -f "/tmp/rust_bench_results.json" ]; then
            echo "### Detailed Results" >> reports/rust/benchmark-summary.md
            echo '```json' >> reports/rust/benchmark-summary.md
            head -c 50000 /tmp/rust_bench_results.json >> reports/rust/benchmark-summary.md
            echo '```' >> reports/rust/benchmark-summary.md
          fi

          cat reports/rust/benchmark-summary.md

      - name: Upload Rust benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: rust-benchmark-results-${{ github.run_number }}
          path: |
            /tmp/rust_bench_results.json
            reports/rust/benchmark-summary.md
            geometry_os/systems/infinite_map_rs/target/criterion/
          retention-days: 90

      - name: Store benchmark data
        if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')
        run: |
          mkdir -p benchmark-data/rust
          cp /tmp/rust_bench_results.json benchmark-data/rust/${{ github.run_number }}.json || true

          # Store for historical comparison
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add benchmark-data/ || true
          git commit -m "chore: store Rust benchmark data for run #${{ github.run_number }}" || true
          git push || true

  # ============================================================================
  # Python Performance Benchmarks
  # ============================================================================
  python-benchmarks:
    name: Python Performance Benchmarks
    runs-on: ubuntu-latest
    if: |
      github.event.inputs.benchmark_suite == 'all' ||
      github.event.inputs.benchmark_suite == 'python' ||
      github.event.inputs.benchmark_suite == ''

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install benchmark dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip install pytest-benchmark memory-profiler psutil
          pip install py-spy  # For profiling

      - name: Run Python benchmarks
        id: python_bench
        run: |
          mkdir -p /tmp/python_bench_results

          # Run benchmarks for key modules
          python3 -c "
          import time
          import json
          import sys
          sys.path.insert(0, 'geometry_os')

          results = {}

          # Benchmark health system
          try:
            from systems.health.software_shm import SoftwareSHM
            start = time.time()
            shm = SoftwareSHM(check_interval=1)
            elapsed = time.time() - start
            results['shm_initialization'] = {
              'time_ms': elapsed * 1000,
              'status': 'success'
            }
          except Exception as e:
            results['shm_initialization'] = {'status': 'error', 'error': str(e)}

          # Save results
          with open('/tmp/python_bench_results/benchmarks.json', 'w') as f:
            json.dump(results, f, indent=2)
          "

      - name: Profile critical paths
        if: github.event.inputs.benchmark_suite == 'all' || github.event.inputs.benchmark_suite == 'python'
        run: |
          # Profile neural substrate if available
          if [ -f "geometry_os/systems/neural_substrate/sse_parser.py" ]; then
            python3 -m pytest geometry_os/tests/unit/test_sse_parser.py \
              --benchmark-only \
              --benchmark-autosave \
              --benchmark-save-data \
              --benchmark-json=/tmp/python_bench_results/sse_parser_bench.json || true
          fi

      - name: Generate Python benchmark report
        if: always()
        run: |
          mkdir -p reports/python

          cat > reports/python/benchmark-summary.md <<EOF
          # Python Performance Benchmarks

          **Workflow Run:** #${{ github.run_number }}
          **Event:** ${{ github.event_name }}
          **Branch:** ${{ github.ref_name }}
          **Commit:** ${{ github.sha }}
          **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")

          ## Benchmark Results

          \`\`\`json
          EOF

          cat /tmp/python_bench_results/benchmarks.json >> reports/python/benchmark-summary.md || echo "{}" >> reports/python/benchmark-summary.md

          echo '```' >> reports/python/benchmark-summary.md
          cat reports/python/benchmark-summary.md

      - name: Upload Python benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: python-benchmark-results-${{ github.run_number }}
          path: |
            /tmp/python_bench_results/
            reports/python/benchmark-summary.md
          retention-days: 90

  # ============================================================================
  # Neural Pipeline Benchmarks
  # ============================================================================
  neural-benchmarks:
    name: Neural Pipeline Benchmarks
    runs-on: ubuntu-latest
    if: |
      github.event.inputs.benchmark_suite == 'all' ||
      github.event.inputs.benchmark_suite == 'neural' ||
      github.event.inputs.benchmark_suite == ''

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip install numpy aiohttp requests pytest-benchmark
          pip install pytest-asyncio

      - name: Run SSE parser benchmarks
        run: |
          mkdir -p /tmp/neural_bench

          # Benchmark SSE parsing
          python3 -c "
          import time
          import asyncio
          import json
          import sys
          sys.path.insert(0, 'geometry_os')

          results = {}

          # Benchmark SSE parser performance
          try:
            from systems.neural_substrate.sse_parser import SSEParser

            async def benchmark_sse():
                iterations = ${{ github.event.inputs.iterations || 10 }}
                times = []

                for i in range(iterations):
                    start = time.time()
                    # Simulate parsing
                    parser = SSEParser('http://localhost:8080/sse')
                    times.append(time.time() - start)

                avg_time = sum(times) / len(times)
                return {
                    'avg_time_ms': avg_time * 1000,
                    'min_time_ms': min(times) * 1000,
                    'max_time_ms': max(times) * 1000,
                    'iterations': iterations
                }

            results['sse_parser'] = asyncio.run(benchmark_sse())
          except Exception as e:
            results['sse_parser'] = {'status': 'error', 'error': str(e)}

          with open('/tmp/neural_bench/sse_benchmark.json', 'w') as f:
            json.dump(results, f, indent=2)
          "

      - name: Generate neural benchmark report
        if: always()
        run: |
          mkdir -p reports/neural

          cat > reports/neural/benchmark-summary.md <<EOF
          # Neural Pipeline Performance Benchmarks

          **Workflow Run:** #${{ github.run_number }}
          **Event:** ${{ github.event_name }}
          **Branch:** ${{ github.ref_name }}
          **Commit:** ${{ github.sha }}
          **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")

          ## SSE Parser Performance

          \`\`\`json
          EOF

          cat /tmp/neural_bench/sse_benchmark.json >> reports/neural/benchmark-summary.md || echo "{}" >> reports/neural/benchmark-summary.md

          echo '```' >> reports/neural/benchmark-summary.md
          cat reports/neural/benchmark-summary.md

      - name: Upload neural benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: neural-benchmark-results-${{ github.run_number }}
          path: |
            /tmp/neural_bench/
            reports/neural/benchmark-summary.md
          retention-days: 90

  # ============================================================================
  # Memory Profiling
  # ============================================================================
  memory-profiling:
    name: Memory Profiling
    runs-on: ubuntu-latest
    if: |
      github.event.inputs.benchmark_suite == 'all' ||
      github.event.inputs.benchmark_suite == 'memory' ||
      github.event.inputs.benchmark_suite == ''

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install profiling tools
        run: |
          python3 -m pip install --upgrade pip
          pip install memory-profiler psutil tracemalloc

      - name: Profile memory usage
        run: |
          mkdir -p /tmp/memory_profile

          python3 <<'PYEOF'
          import sys
          import tracemalloc
          import json
          sys.path.insert(0, 'geometry_os')

          tracemalloc.start()

          # Profile health system
          try:
              from systems.health.software_shm import SoftwareSHM
              snapshot1 = tracemalloc.take_snapshot()
              shm = SoftwareSHM()
              snapshot2 = tracemalloc.take_snapshot()

              top_stats = snapshot2.compare_to(snapshot1, 'lineno')
              results = []
              for stat in top_stats[:10]:
                  results.append({
                      'file': stat.traceback[0].filename,
                      'line': stat.traceback[0].lineno,
                      'size_diff_kb': stat.size_diff / 1024,
                      'size_kb': stat.size / 1024
                  })

              with open('/tmp/memory_profile/shm_memory.json', 'w') as f:
                  json.dump(results, f, indent=2)
          except Exception as e:
              with open('/tmp/memory_profile/shm_memory.json', 'w') as f:
                  json.dump({'error': str(e)}, f)

          tracemalloc.stop()
          PYEOF

      - name: Generate memory profile report
        if: always()
        run: |
          mkdir -p reports/memory

          cat > reports/memory/profile-summary.md <<EOF
          # Memory Profiling Results

          **Workflow Run:** #${{ github.run_number }}
          **Event:** ${{ github.event_name }}
          **Branch:** ${{ github.ref_name }}
          **Commit:** ${{ github.sha }}
          **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")

          ## Health System Memory Profile

          \`\`\`json
          EOF

          cat /tmp/memory_profile/shm_memory.json >> reports/memory/profile-summary.md || echo "{}" >> reports/memory/profile-summary.md

          echo '```' >> reports/memory/profile-summary.md
          cat reports/memory/profile-summary.md

      - name: Upload memory profile results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: memory-profile-results-${{ github.run_number }}
          path: |
            /tmp/memory_profile/
            reports/memory/profile-summary.md
          retention-days: 90

  # ============================================================================
  # Performance Regression Analysis
  # ============================================================================
  performance-analysis:
    name: Performance Regression Analysis
    runs-on: ubuntu-latest
    needs: [rust-benchmarks, python-benchmarks, neural-benchmarks, memory-profiling]
    if: always()

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all benchmark results
        uses: actions/download-artifact@v4

      - name: Analyze performance trends
        run: |
          mkdir -p reports/analysis

          cat > reports/analysis/performance-analysis.md <<EOF
          # Performance Benchmark Analysis

          **Workflow Run:** #${{ github.run_number }}
          **Event:** ${{ github.event_name }}
          **Branch:** ${{ github.ref_name }}
          **Commit:** ${{ github.sha }}
          **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")

          ## Benchmark Suite Status

          - Rust Benchmarks: ${{ needs.rust-benchmarks.result }}
          - Python Benchmarks: ${{ needs.python-benchmarks.result }}
          - Neural Benchmarks: ${{ needs.neural-benchmarks.result }}
          - Memory Profiling: ${{ needs.memory-profiling.result }}

          ## Performance Summary

          ### Key Metrics

          EOF

          # Add performance trend analysis if historical data exists
          if [ -d "benchmark-data" ]; then
            echo "### Historical Comparison" >> reports/analysis/performance-analysis.md
            echo "" >> reports/analysis/performance-analysis.md
            echo "Historical benchmark data available for comparison." >> reports/analysis/performance-analysis.md
          fi

          cat reports/analysis/performance-analysis.md

      - name: Upload performance analysis
        uses: actions/upload-artifact@v4
        with:
          name: performance-analysis-${{ github.run_number }}
          path: reports/analysis/
          retention-days: 90

      - name: Comment on PR with performance results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let prComment = '# Performance Benchmark Results\n\n';

            // Add summary for each benchmark suite
            const suites = [
              { name: 'Rust', result: '${{ needs.rust-benchmarks.result }}' },
              { name: 'Python', result: '${{ needs.python-benchmarks.result }}' },
              { name: 'Neural', result: '${{ needs.neural-benchmarks.result }}' },
              { name: 'Memory', result: '${{ needs.memory-profiling.result }}' }
            ];

            suites.forEach(suite => {
              const emoji = suite.result === 'success' ? '✅' : '❌';
              prComment += `${emoji} **${suite.name} Benchmarks**: ${suite.result}\n`;
            });

            prComment += '\n---\n';
            prComment += '*Detailed results are available in the workflow artifacts.*\n';

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: prComment
            });

      - name: Check for performance regression
        if: github.event_name == 'pull_request'
        run: |
          # This is a placeholder for regression detection logic
          # In production, you would compare against baseline benchmarks
          echo "Checking for performance regressions..."
          echo "No regression thresholds configured yet"
