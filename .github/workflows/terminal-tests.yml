name: Terminal Tests

on:
  # Trigger on push to main branches
  push:
    branches:
      - main
      - master
    paths:
      - 'systems/visual_shell/api/**'
      - 'wordpress_zone/wordpress/wp-content/plugins/geometry-os-web-terminal/**'
      - '.github/workflows/terminal-tests.yml'

  # Trigger on pull requests
  pull_request:
    branches:
      - main
      - master
    paths:
      - 'systems/visual_shell/api/**'
      - 'wordpress_zone/wordpress/wp-content/plugins/geometry-os-web-terminal/**'
      - '.github/workflows/terminal-tests.yml'

  # Trigger nightly at 03:00 UTC
  schedule:
    - cron: '0 3 * * *'

  # Allow manual workflow dispatch
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run (all, websocket, e2e, llm)'
        required: false
        type: choice
        options:
          - all
          - websocket
          - e2e
          - llm
        default: 'all'
      verbose:
        description: 'Run with verbose output'
        required: false
        type: boolean
        default: false

jobs:
  # ============================================================================
  # WebSocket Tests
  # ============================================================================
  websocket-tests:
    name: WebSocket Tests
    runs-on: ubuntu-latest
    if: |
      github.event.inputs.test_suite == 'all' ||
      github.event.inputs.test_suite == 'websocket' ||
      github.event.inputs.test_suite == ''

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip install pytest pytest-asyncio pytest-cov aiohttp websockets

      - name: Run WebSocket tests
        id: websocket_tests
        run: |
          pytest systems/visual_shell/api/tests/test_terminal_bridge.py \
            systems/visual_shell/api/tests/test_token_relay.py \
            -v --tb=short \
            --junit-xml=test-results/websocket-tests.xml \
            --cov=systems/visual_shell/api \
            --cov-report=xml \
            --cov-report=term-missing
        continue-on-error: true

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: websocket-test-results-${{ github.run_number }}
          path: |
            test-results/
            coverage.xml
          retention-days: 30

  # ============================================================================
  # E2E Tests (Puppeteer)
  # ============================================================================
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    if: |
      github.event.inputs.test_suite == 'all' ||
      github.event.inputs.test_suite == 'e2e' ||
      github.event.inputs.test_suite == ''

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install Puppeteer
        run: |
          npm install puppeteer
        continue-on-error: true

      - name: Run E2E tests
        id: e2e_tests
        run: |
          if [ -f "systems/visual_shell/api/tests/e2e_terminal_test.js" ]; then
            node systems/visual_shell/api/tests/e2e_terminal_test.js
          else
            echo "E2E test file not found, skipping..."
            echo "::warning::E2E test file not found"
          fi
        continue-on-error: true

      - name: Upload screenshots
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-screenshots-${{ github.run_number }}
          path: |
            test-results/screenshots/
            screenshots/
          retention-days: 30
          if-no-files-found: ignore

  # ============================================================================
  # LLM Tests (Mock Mode)
  # ============================================================================
  llm-tests:
    name: LLM Tests
    runs-on: ubuntu-latest
    if: |
      github.event.inputs.test_suite == 'all' ||
      github.event.inputs.test_suite == 'llm' ||
      github.event.inputs.test_suite == ''

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip install pytest aiohttp requests

      - name: Run LLM tests (mock mode)
        id: llm_tests
        env:
          TERMINAL_TEST_MOCK_LLM: "1"
          LM_STUDIO_URL: "mock://disabled"
        run: |
          python3 -c "
          import sys
          import os
          sys.path.insert(0, 'systems/visual_shell/api/tests')

          print('Running LLM tests in mock mode...')

          try:
              from llm_terminal_verify import VerificationResult, LLMTerminalVerifier, TerminalTestRunner
              print('  - Module imports: OK')

              # Test data structure
              result = VerificationResult(
                  test_name='mock_test',
                  passed=True,
                  llm_response='mock response',
                  confidence=0.95,
                  details='Mock test passed'
              )
              assert result.passed == True
              print('  - Data structures: OK')

              # Test mock mode is active
              assert os.environ.get('TERMINAL_TEST_MOCK_LLM') == '1'
              print('  - Mock mode: ENABLED')

              print('LLM mock tests passed')
              sys.exit(0)
          except Exception as e:
              print(f'LLM mock tests failed: {e}')
              sys.exit(1)
          "
        continue-on-error: true

      - name: Upload LLM test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: llm-test-results-${{ github.run_number }}
          path: |
            test-results/llm/
          retention-days: 30
          if-no-files-found: ignore

  # ============================================================================
  # Unified Test Runner (CI Script)
  # ============================================================================
  unified-tests:
    name: Unified Test Runner
    runs-on: ubuntu-latest
    needs: [websocket-tests, e2e-tests, llm-tests]
    if: always()

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip install pytest pytest-asyncio aiohttp websockets

      - name: Download all test results
        uses: actions/download-artifact@v4

      - name: Run CI terminal test script
        id: ci_tests
        run: |
          chmod +x systems/visual_shell/api/tests/ci_terminal_tests.sh
          ./systems/visual_shell/api/tests/ci_terminal_tests.sh
        continue-on-error: true

      - name: Generate test summary
        run: |
          mkdir -p reports

          cat > reports/terminal-test-summary.md <<EOF
          # Terminal Test Summary

          **Workflow Run:** #${{ github.run_number }}
          **Event:** ${{ github.event_name }}
          **Branch:** ${{ github.ref_name }}
          **Commit:** ${{ github.sha }}
          **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")

          ## Test Results

          ### WebSocket Tests
          - Status: ${{ needs.websocket-tests.result }}
          - Suite: Terminal bridge, token relay

          ### E2E Tests
          - Status: ${{ needs.e2e-tests.result }}
          - Suite: Puppeteer browser tests

          ### LLM Tests
          - Status: ${{ needs.llm-tests.result }}
          - Suite: LLM verification (mock mode)

          ## Overall Status
          EOF

          if [ "${{ needs.websocket-tests.result }}" == "success" ] && \
             [ "${{ needs.e2e-tests.result }}" != "failure" ] && \
             [ "${{ needs.llm-tests.result }}" == "success" ]; then
            echo "- Status: PASSED" >> reports/terminal-test-summary.md
            echo "" >> reports/terminal-test-summary.md
            echo "All terminal tests passed successfully!" >> reports/terminal-test-summary.md
          else
            echo "- Status: NEEDS REVIEW" >> reports/terminal-test-summary.md
            echo "" >> reports/terminal-test-summary.md
            echo "Some test suites may have issues. Please review the logs." >> reports/terminal-test-summary.md
          fi

          cat reports/terminal-test-summary.md

      - name: Upload test summary
        uses: actions/upload-artifact@v4
        with:
          name: terminal-test-summary-${{ github.run_number }}
          path: reports/terminal-test-summary.md
          retention-days: 90

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('reports/terminal-test-summary.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

      - name: Notify on failure
        if: failure() && github.event_name == 'schedule'
        run: |
          echo "::warning::Scheduled terminal tests failed!"
          echo "Branch: ${{ github.ref_name }}"
          echo "Commit: ${{ github.sha }}"
