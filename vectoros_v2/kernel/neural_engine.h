// VectorOS v2 Neural Engine Header
// Generated by v1 Neural OS Bootstrap System

#pragma once

#include <memory>
#include <string>
#include <vector>
#include <unordered_map>
#include <chrono>

namespace vectoros_v2 {

// Forward declarations
class MemoryManager;
class Tensor;

/**
 * @brief Neural Engine - Handles neural network operations and GGUF model loading
 * 
 * Implements GPU-accelerated neural inference with unified tensor memory management.
 * Supports GGUF model compatibility and real-time neural processing.
 */
class NeuralEngine {
private:
    MemoryManager* memory_manager_;
    std::unordered_map<std::string, std::shared_ptr<Tensor>> model_cache_;
    
    bool initialized_ = false;
    uint64_t total_inference_time_ = 0;
    uint64_t inference_count_ = 0;
    
public:
    NeuralEngine(MemoryManager* memory_manager);
    ~NeuralEngine() = default;
    
    /**
     * @brief Initialize the neural engine
     * @return true if initialization successful, false otherwise
     */
    bool initialize();
    
    /**
     * @brief Load GGUF model from file
     * @param model_path Path to GGUF model file
     * @param model_name Name to assign to the loaded model
     * @return true if model loaded successfully, false otherwise
     */
    bool load_model(const std::string& model_path, const std::string& model_name);
    
    /**
     * @brief Execute neural inference
     * @param model_name Name of the model to use
     * @param input Input tensor
     * @param output Output tensor (will be allocated if null)
     * @return true if inference successful, false otherwise
     */
    bool execute_inference(const std::string& model_name, 
                          const Tensor& input, 
                          Tensor& output);
    
    /**
     * @brief Get model information
     * @param model_name Name of the model
     * @return Model information string
     */
    std::string get_model_info(const std::string& model_name) const;
    
    /**
     * @brief Get average inference time
     * @return Average inference time in microseconds
     */
    uint64_t get_average_inference_time() const;
    
    /**
     * @brief Get inference count
     * @return Total number of inferences performed
     */
    uint64_t get_inference_count() const { return inference_count_; }
    
    /**
     * @brief Clear model cache
     */
    void clear_cache();
    
    /**
     * @brief Check if engine is initialized
     * @return true if initialized, false otherwise
     */
    bool is_initialized() const { return initialized_; }
    
    /**
     * @brief Get memory usage of neural engine
     * @return Memory usage in bytes
     */
    size_t get_memory_usage() const;
};

/**
 * @brief Tensor - Unified tensor representation for neural operations
 * 
 * Represents multi-dimensional arrays with GPU memory support.
 * Integrates with the unified memory management system.
 */
class Tensor {
private:
    std::vector<size_t> shape_;
    size_t element_size_ = 0;
    size_t total_size_ = 0;
    void* data_ = nullptr;
    bool on_gpu_ = false;
    
public:
    Tensor() = default;
    Tensor(const std::vector<size_t>& shape, size_t element_size);
    ~Tensor();
    
    /**
     * @brief Allocate tensor memory
     * @param memory_manager Memory manager to use for allocation
     * @param on_gpu Whether to allocate on GPU memory
     * @return true if allocation successful, false otherwise
     */
    bool allocate(MemoryManager& memory_manager, bool on_gpu = false);
    
    /**
     * @brief Deallocate tensor memory
     * @param memory_manager Memory manager to use for deallocation
     */
    void deallocate(MemoryManager& memory_manager);
    
    /**
     * @brief Get tensor shape
     * @return Vector of dimensions
     */
    const std::vector<size_t>& get_shape() const { return shape_; }
    
    /**
     * @brief Get element size in bytes
     * @return Element size
     */
    size_t get_element_size() const { return element_size_; }
    
    /**
     * @brief Get total size in bytes
     * @return Total size
     */
    size_t get_total_size() const { return total_size_; }
    
    /**
     * @brief Get data pointer
     * @return Pointer to tensor data
     */
    void* get_data() const { return data_; }
    
    /**
     * @brief Check if tensor is on GPU
     * @return true if on GPU, false if on CPU
     */
    bool is_on_gpu() const { return on_gpu_; }
    
    /**
     * @brief Copy data from another tensor
     * @param other Source tensor
     * @param memory_manager Memory manager for operations
     * @return true if copy successful, false otherwise
     */
    bool copy_from(const Tensor& other, MemoryManager& memory_manager);
    
    /**
     * @brief Move data to GPU
     * @param memory_manager Memory manager for operations
     * @return true if move successful, false otherwise
     */
    bool move_to_gpu(MemoryManager& memory_manager);
    
    /**
     * @brief Move data to CPU
     * @param memory_manager Memory manager for operations
     * @return true if move successful, false otherwise
     */
    bool move_to_cpu(MemoryManager& memory_manager);
};

} // namespace vectoros_v2