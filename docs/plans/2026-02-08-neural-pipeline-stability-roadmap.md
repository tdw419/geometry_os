# Neural Pipeline Stability & Performance Enhancement Roadmap

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Systematically improve the Geometry OS neural/LLM bridge system through immediate fixes, stability enhancements, scalability improvements, and long-term GPU acceleration.

**Architecture:** The neural pipeline consists of:
1. **LM Studio Bridge** (`systems/neural_substrate/lm_studio_bridge.py`) - SSE streaming client
2. **Compositor** (`systems/infinite_map_rs/src/app.rs`) - Rust rendering engine
3. **Software SHM** (`systems/health/software_shm.py`) - Health monitoring system
4. **WebSocket** - Real-time neural state broadcasting

**Tech Stack:** Python (asyncio, aiohttp), Rust (wgpu, WebSocket), Unix sockets, SSE (Server-Sent Events), LM Studio API

**Current Status:**
- ‚úÖ Rust compilation fixed (duplicate definitions removed)
- ‚úÖ LM Studio Bridge upgraded to aiohttp for non-blocking SSE streaming
- ‚úÖ Software SHM CLI now accepts --check argument
- ‚úÖ Neural state broadcasting confirmed in app.rs
- ‚ö†Ô∏è System health reports "critical (symmetry)"

---

# Phase 1: Immediate Fixes & Validation (1-3 days)

## Task 1: Investigate "Critical (Symmetry)" Health Issue

**Files:**
- Read: `geometry_os/systems/health/software_shm.py` (already reviewed)
- Modify: `geometry_os/systems/health/software_shm.py` (add verbose logging)
- Create: `geometry_os/systems/health/shm_debug.py` (debugging helper)

**Step 1: Run health check with verbose output**

Run: `cd geometry_os/systems/health && python3 software_shm.py --check 2>&1 | tee /tmp/shm_debug.log`

Expected: Current output showing "critical (symmetry)"

**Step 2: Create debugging helper script**

Create file: `geometry_os/systems/health/shm_debug.py`

```python
#!/usr/bin/env python3
"""
SHM Debug Helper - Detailed symmetry analysis
"""
import sys
import json
from pathlib import Path
from collections import defaultdict

# Add parent to path
sys.path.insert(0, str(Path(__file__).parent.parent))
from health.software_shm import SoftwareSHM

def debug_symmetry(project_root: str = None):
    """Debug symmetry score calculation"""
    shm = SoftwareSHM(project_root=project_root)

    print("\n" + "=" * 70)
    print("SYMMETRY DEBUG REPORT")
    print("=" * 70)

    # Directory structure analysis
    systems_dir = shm.project_root / 'systems'
    if systems_dir.exists():
        subdirs = [d for d in systems_dir.iterdir() if d.is_dir()]

        print(f"\nüìÅ Systems directory: {len(subdirs)} subdirectories")

        # Size distribution
        sizes = []
        dir_details = []
        for subdir in subdirs:
            size = sum(1 for _ in subdir.rglob('*') if _.is_file())
            sizes.append(size)
            dir_details.append((subdir.name, size))

        # Sort by size
        dir_details.sort(key=lambda x: x[1], reverse=True)

        print("\nüìä Top 20 largest subsystems:")
        for name, size in dir_details[:20]:
            bar = "‚ñà" * (size // 10)
            print(f"  {name:30s} {size:6d} files {bar}")

        print(f"\nüìà Size distribution:")
        print(f"  Total files: {sum(sizes)}")
        print(f"  Average: {sum(sizes) / len(sizes):.1f}")
        print(f"  Median: {sorted(sizes)[len(sizes) // 2]}")
        print(f"  Max: {max(sizes)} ({dir_details[0][0]})")
        print(f"  Min: {min(sizes)} ({dir_details[-1][0]})")

        # Balance score
        import numpy as np
        if sizes:
            total = sum(sizes)
            if total > 0:
                normalized = [s / total for s in sizes]
                entropy = -sum(p * np.log2(p) if p > 0 else 0 for p in normalized)
                max_entropy = np.log2(len(sizes))
                balance_score = entropy / max_entropy if max_entropy > 0 else 1.0

                print(f"\n‚öñÔ∏è  Balance score: {balance_score:.4f}")
                print(f"  Entropy: {entropy:.4f} / {max_entropy:.4f}")

                if balance_score < 0.7:
                    print("  ‚ö†Ô∏è  LOW BALANCE - Directory structure is imbalanced")
                    print("  üí° Consider: Consolidating or splitting subsystems")

    # Circular dependency check
    print("\nüîÑ Circular dependency analysis:")
    rust_files = list(shm.project_root.rglob('*.rs'))
    circular_risk = 0
    risky_files = []

    for rust_file in rust_files:
        try:
            content = rust_file.read_text()
            mod_count = content.count('mod ')
            super_count = content.count('use super::')
            if mod_count > 0 and super_count > 0:
                circular_risk += 1
                if len(risky_files) < 10:
                    rel_path = rust_file.relative_to(shm.project_root)
                    risky_files.append(str(rel_path))
        except:
            pass

    print(f"  Files with 'mod' and 'use super::': {circular_risk}")
    if risky_files:
        print("  ‚ö†Ô∏è  Potential circular dependency patterns:")
        for f in risky_files:
            print(f"    - {f}")

    # Recommendations
    print("\n" + "=" * 70)
    print("RECOMMENDATIONS")
    print("=" * 70)

    if balance_score < 0.7:
        print("\n1. Address directory imbalance:")
        print("   - Consider moving smaller subsystems into categories")
        print("   - Split very large subsystems into modules")

    if circular_risk > len(rust_files) * 0.1:
        print("\n2. Reduce circular dependency risk:")
        print("   - Refactor 'use super::*' patterns")
        print("   - Use explicit module paths")
        print("   - Consider dependency injection")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--project-root", type=str, default=None)
    args = parser.parse_args()

    debug_symmetry(args.project_root)
```

**Step 3: Run debug helper**

Run: `cd geometry_os && python3 systems/health/shm_debug.py`

Expected: Detailed analysis of what's causing low symmetry score

**Step 4: Document findings**

Create file: `geometry_os/docs/health/symmetry-analysis.md`

```markdown
# SHM Symmetry Analysis

Date: 2026-02-08

## Current Status
- Overall PAS Score: See below
- Symmetry Dimension: CRITICAL

## Root Cause Analysis

### Directory Imbalance
[Fill in after running debug script]

### Circular Dependencies
[Fill in after running debug script]

## Proposed Fixes

### Phase 1 (Quick Wins)
- [ ] Consolidate small subsystems
- [ ] Add documentation to improve consistency score

### Phase 2 (Structural)
- [ ] Refactor module structure
- [ ] Reduce circular dependencies

## References
- SHM Implementation: `systems/health/software_shm.py`
- Debug Tool: `systems/health/shm_debug.py`
```

**Step 5: Commit**

```bash
git add geometry_os/systems/health/shm_debug.py
git add geometry_os/docs/health/symmetry-analysis.md
git commit -m "feat(health): add SHM debugging tools and analysis"
```

---

## Task 2: Test Full Neural ‚Üí Visual Pipeline

**Files:**
- Read: `geometry_os/systems/neural_substrate/lm_studio_bridge.py` (already reviewed)
- Read: `geometry_os/systems/infinite_map_rs/src/api_server.rs` (WebSocket endpoint)
- Create: `geometry_os/tests/integration/test_neural_pipeline.py`

**Step 1: Create integration test**

Create file: `geometry_os/tests/integration/test_neural_pipeline.py`

```python
#!/usr/bin/env python3
"""
Integration test for Neural ‚Üí Visual Pipeline

Tests the full flow:
1. LM Studio generates tokens
2. Bridge streams tokens via SSE
3. Tokens forwarded to compositor socket
4. Compositor broadcasts via WebSocket
5. Visual shell receives updates
"""
import asyncio
import json
import socket
import time
import sys
from pathlib import Path

# Add project root
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from systems.neural_substrate.lm_studio_bridge import LMStudioBridge


class TestResults:
    def __init__(self):
        self.passed = []
        self.failed = []
        self.warnings = []

    def add_pass(self, test_name: str):
        self.passed.append(test_name)
        print(f"‚úÖ {test_name}")

    def add_fail(self, test_name: str, error: str):
        self.failed.append((test_name, error))
        print(f"‚ùå {test_name}: {error}")

    def add_warning(self, message: str):
        self.warnings.append(message)
        print(f"‚ö†Ô∏è  {message}")

    def summary(self):
        print("\n" + "=" * 70)
        print("TEST SUMMARY")
        print("=" * 70)
        print(f"Passed: {len(self.passed)}")
        print(f"Failed: {len(self.failed)}")
        print(f"Warnings: {len(self.warnings)}")

        if self.failed:
            print("\nFailed Tests:")
            for name, error in self.failed:
                print(f"  - {name}: {error}")

        return len(self.failed) == 0


async def test_lm_studio_connection(results: TestResults):
    """Test 1: Verify LM Studio is accessible"""
    try:
        import requests

        response = requests.get("http://localhost:1234/v1/models", timeout=5)
        if response.status_code == 200:
            models = response.json().get("data", [])
            results.add_pass("LM Studio Connection")
            if models:
                results.add_pass(f"Model loaded: {models[0].get('id', 'unknown')}")
            else:
                results.add_warning("LM Studio connected but no model loaded")
            return True
        else:
            results.add_fail("LM Studio Connection", f"Status {response.status_code}")
            return False
    except Exception as e:
        results.add_fail("LM Studio Connection", str(e))
        return False


async def test_sse_streaming(results: TestResults):
    """Test 2: Verify SSE streaming works"""
    try:
        if not HAS_AIOHTTP:
            results.add_fail("SSE Streaming", "aiohttp not installed")
            return False

        import aiohttp

        async with aiohttp.ClientSession() as session:
            async with session.post(
                "http://localhost:1234/v1/chat/completions",
                json={
                    "messages": [{"role": "user", "content": "Say 'test'"}],
                    "stream": True,
                    "max_tokens": 10
                },
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                if response.status != 200:
                    results.add_fail("SSE Streaming", f"Status {response.status}")
                    return False

                token_count = 0
                async for line in response.content:
                    line = line.decode('utf-8').strip()
                    if line.startswith('data: ') and line != 'data: [DONE]':
                        token_count += 1
                        if token_count >= 1:  # Got at least one token
                            break

                if token_count > 0:
                    results.add_pass(f"SSE Streaming ({token_count} tokens)")
                    return True
                else:
                    results.add_fail("SSE Streaming", "No tokens received")
                    return False
    except Exception as e:
        results.add_fail("SSE Streaming", str(e))
        return False


async def test_compositor_socket(results: TestResults):
    """Test 3: Verify compositor socket receives messages"""
    socket_path = "/tmp/evolution_daemon.sock"

    try:
        # Try to connect
        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        sock.settimeout(1)
        sock.connect(socket_path)
        sock.close()

        results.add_pass("Compositor Socket Accessible")
        return True
    except FileNotFoundError:
        results.add_warning("Compositor socket not found - compositor not running?")
        return None  # Not a failure if compositor isn't running
    except Exception as e:
        results.add_fail("Compositor Socket", str(e))
        return False


async def test_token_forwarding(results: TestResults):
    """Test 4: Verify tokens are forwarded to compositor"""
    try:
        bridge = LMStudioBridge()

        # Check if we can send a test message
        test_message = bridge.create_token_message("test", 12345, 1.0, 0)

        # Try to send
        try:
            await bridge.send_token_activation("test", 12345, 1.0, 0)
            results.add_pass("Token Forwarding")
            return True
        except FileNotFoundError:
            results.add_warning("Cannot send token - compositor socket not available")
            return None
        except Exception as e:
            results.add_fail("Token Forwarding", str(e))
            return False
    except Exception as e:
        results.add_fail("Token Forwarding", str(e))
        return False


async def test_neural_state_broadcasting(results: TestResults):
    """Test 5: Verify neural state messages can be created"""
    try:
        bridge = LMStudioBridge()
        message = bridge.create_neural_state_message(0.8)

        # Validate message structure
        assert message["message_type"] == "NeuralStateUpdate"
        assert "payload" in message
        assert "daemon_id" in message["payload"]
        assert "data" in message["payload"]

        results.add_pass("Neural State Message Structure")
        return True
    except Exception as e:
        results.add_fail("Neural State Broadcasting", str(e))
        return False


async def test_websocket_connection(results: TestResults):
    """Test 6: Verify WebSocket for visual shell (if compositor running)"""
    try:
        import websockets

        ws_url = "ws://localhost:8765/neural"

        async with websockets.connect(ws_url, timeout=2) as ws:
            results.add_pass("WebSocket Connection")
            return True
    except ImportError:
        results.add_warning("websockets library not installed - skipping WebSocket test")
        return None
    except Exception as e:
        results.add_warning(f"WebSocket not available: {e} (compositor may not be running)")
        return None


async def run_all_tests():
    """Run all integration tests"""
    results = TestResults()

    print("\n" + "=" * 70)
    print("NEURAL ‚Üí VISUAL PIPELINE INTEGRATION TESTS")
    print("=" * 70)
    print()

    # Test 1: LM Studio Connection
    await test_lm_studio_connection(results)
    print()

    # Test 2: SSE Streaming
    await test_sse_streaming(results)
    print()

    # Test 3: Compositor Socket
    await test_compositor_socket(results)
    print()

    # Test 4: Token Forwarding
    await test_token_forwarding(results)
    print()

    # Test 5: Neural State Broadcasting
    await test_neural_state_broadcasting(results)
    print()

    # Test 6: WebSocket
    await test_websocket_connection(results)
    print()

    # Summary
    success = results.summary()

    # Return exit code
    return 0 if success else 1


if __name__ == "__main__":
    exit_code = asyncio.run(run_all_tests())
    sys.exit(exit_code)
```

**Step 2: Run integration tests**

Run: `cd geometry_os && python3 tests/integration/test_neural_pipeline.py`

Expected: Some tests may pass, others may warn if compositor isn't running

**Step 3: Create compositor startup script**

Create file: `geometry_os/scripts/start_compositor.sh`

```bash
#!/bin/bash
# Start the Infinite Map compositor for neural pipeline testing

set -e

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
cd "$PROJECT_ROOT/geometry_os/systems/infinite_map_rs"

echo "Starting Infinite Map compositor..."
echo "Project root: $PROJECT_ROOT"
echo ""

# Check if built
if [ ! -f "target/release/infinite_map_rs" ]; then
    echo "Building infinite_map_rs..."
    cargo build --release
fi

# Start with WebSocket enabled
echo "Launching compositor..."
RUST_LOG=info ./target/release/infinite_map_rs \
    --websocket-port 8765 \
    --daemon-socket /tmp/evolution_daemon.sock \
    2>&1 | tee /tmp/compositor.log &

COMPOSITOR_PID=$!
echo "Compositor PID: $COMPOSITOR_PID"
echo "Socket: /tmp/evolution_daemon.sock"
echo "WebSocket: ws://localhost:8765/neural"
echo ""
echo "Waiting for startup..."
sleep 3

# Check if still running
if ps -p $COMPOSITOR_PID > /dev/null; then
    echo "‚úÖ Compositor running"
    echo $COMPOSITOR_PID > /tmp/compositor.pid
else
    echo "‚ùå Compositor failed to start"
    exit 1
fi
```

**Step 4: Create compositor stop script**

Create file: `geometry_os/scripts/stop_compositor.sh`

```bash
#!/bin/bash
# Stop the Infinite Map compositor

if [ -f "/tmp/compositor.pid" ]; then
    PID=$(cat /tmp/compositor.pid)
    if ps -p $PID > /dev/null; then
        echo "Stopping compositor (PID: $PID)..."
        kill $PID
        rm /tmp/compositor.pid
        echo "‚úÖ Stopped"
    else
        echo "Compositor not running"
        rm /tmp/compositor.pid
    fi
else
    echo "No PID file found"
fi

# Clean up socket
rm -f /tmp/evolution_daemon.sock
```

**Step 5: Create end-to-end test script**

Create file: `geometry_os/scripts/test_e2e_pipeline.sh`

```bash
#!/bin/bash
# End-to-end test of neural ‚Üí visual pipeline

set -e

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
cd "$PROJECT_ROOT"

echo "=========================================="
echo "NEURAL ‚Üí VISUAL PIPELINE E2E TEST"
echo "=========================================="
echo ""

# Step 1: Start compositor
echo "[1/4] Starting compositor..."
bash geometry_os/scripts/start_compositor.sh
sleep 5
echo ""

# Step 2: Run integration tests
echo "[2/4] Running integration tests..."
python3 geometry_os/tests/integration/test_neural_pipeline.py
TEST_RESULT=$?
echo ""

# Step 3: Test with actual LM Studio inference
echo "[3/4] Testing LM Studio inference..."
echo "Sending test prompt to LM Studio..."
python3 - <<'EOF'
import asyncio
import sys
sys.path.insert(0, 'geometry_os')

from systems.neural_substrate.lm_studio_bridge import LMStudioBridge

async def test_inference():
    bridge = LMStudioBridge()

    print("Sending: 'Hello, Geometry OS!'")
    # This would connect to actual LM Studio
    # For now, just verify the bridge works

    message = bridge.create_token_message("Hello", 100, 1.0, 0)
    print(f"Created message: {message['message_type']}")
    print("‚úÖ Bridge functional")

asyncio.run(test_inference())
EOF
echo ""

# Step 4: Stop compositor
echo "[4/4] Stopping compositor..."
bash geometry_os/scripts/stop_compositor.sh
echo ""

echo "=========================================="
echo "E2E TEST COMPLETE"
echo "=========================================="

if [ $TEST_RESULT -eq 0 ]; then
    echo "‚úÖ All tests passed"
    exit 0
else
    echo "‚ùå Some tests failed"
    exit 1
fi
```

**Step 6: Make scripts executable**

Run: `chmod +x geometry_os/scripts/start_compositor.sh geometry_os/scripts/stop_compositor.sh geometry_os/scripts/test_e2e_pipeline.sh`

**Step 7: Commit**

```bash
git add geometry_os/tests/integration/test_neural_pipeline.py
git add geometry_os/scripts/start_compositor.sh
git add geometry_os/scripts/stop_compositor.sh
git add geometry_os/scripts/test_e2e_pipeline.sh
git commit -m "test: add neural pipeline integration tests and compositor control scripts"
```

---

## Task 3: Clean Up Rust Warnings

**Files:**
- Read: `geometry_os/systems/infinite_map_rs/src/app.rs` (check for unused variables)
- Modify: `geometry_os/systems/infinite_map_rs/src/app.rs` (remove unused variables)

**Step 1: Run clippy to find warnings**

Run: `cd geometry_os/systems/infinite_map_rs && cargo clippy 2>&1 | grep "warning:" | head -20`

Expected: List of unused variable warnings

**Step 2: Fix unused variables**

For each unused variable warning:
- If it's truly unused, remove it
- If it's needed for debugging, prefix with `_`
- If it's used indirectly, ensure the compiler can see the usage

Example fixes:
```rust
// Before
let gpu_caps = self.gpu_caps.clone();

// After (if truly unused)
// Remove the line entirely

// Or (if keeping for future use)
let _gpu_caps = self.gpu_caps.clone();
```

**Step 3: Run clippy again to verify**

Run: `cd geometry_os/systems/infinite_map_rs && cargo clippy`

Expected: No warnings (or only acceptable ones)

**Step 4: Commit**

```bash
git add geometry_os/systems/infinite_map_rs/src/app.rs
git commit -m "refactor: remove unused variables in app.rs"
```

---

# Phase 2: Stability & Performance (3-7 days)

## Task 4: Optimize LM Studio Bridge

**Files:**
- Modify: `geometry_os/systems/neural_substrate/lm_studio_bridge.py`
- Create: `geometry_os/systems/neural_substrate/sse_parser.py`
- Create: `geometry_os/tests/unit/test_sse_parser.py`

**Step 1: Create SSE parser module**

Create file: `geometry_os/systems/neural_substrate/sse_parser.py`

```python
#!/usr/bin/env python3
"""
Server-Sent Events (SSE) Parser

Robust SSE parsing with error handling and reconnection support.
"""
import re
import asyncio
from typing import AsyncIterator, Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)


class SSEParseError(Exception):
    """Raised when SSE parsing fails"""
    pass


class SSEEvent:
    """Represents a single SSE event"""

    def __init__(self):
        self.id: Optional[str] = None
        self.event: Optional[str] = None
        self.data: str = ""
        self.retry: Optional[int] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "event": self.event,
            "data": self.data,
            "retry": self.retry
        }


class SSEParser:
    """
    Parser for Server-Sent Events (SSE).

    Handles:
    - Multi-line data events
    - Event types
    - Retry intervals
    - Event IDs
    - Connection resets
    """

    # SSE field pattern
    FIELD_PATTERN = re.compile(r'^(\w+):\s*(.*)$')

    def __init__(self):
        self.current_event = SSEEvent()
        self.last_event_id: Optional[str] = None
        self.retry_interval: int = 3000  # Default 3 seconds

    def feed_line(self, line: str) -> Optional[SSEEvent]:
        """
        Feed a line to the parser.

        Returns an SSEEvent when a complete event is received,
        or None if more lines are needed.

        Args:
            line: A single line from the SSE stream

        Returns:
            SSEEvent if complete, None otherwise

        Raises:
            SSEParseError: If the line is malformed
        """
        # Empty line signals end of event
        if not line.strip():
            if self.current_event.data or self.current_event.event:
                event = self.current_event
                self.current_event = SSEEvent()
                return event
            return None

        # Comment line - ignore
        if line.startswith(':'):
            return None

        # Parse field
        match = self.FIELD_PATTERN.match(line)
        if not match:
            logger.warning(f"Malformed SSE line: {line}")
            return None

        field_name, field_value = match.groups()

        # Handle different field types
        if field_name == 'data':
            # Append to existing data with newline
            if self.current_event.data:
                self.current_event.data += '\n'
            self.current_event.data += field_value

        elif field_name == 'event':
            self.current_event.event = field_value

        elif field_name == 'id':
            self.current_event.id = field_value
            self.last_event_id = field_value

        elif field_name == 'retry':
            try:
                self.current_event.retry = int(field_value)
                self.retry_interval = self.current_event.retry
            except ValueError:
                logger.warning(f"Invalid retry value: {field_value}")

        return None

    def reset(self):
        """Reset the parser state"""
        self.current_event = SSEEvent()


class SSEClient:
    """
    Async SSE client with auto-reconnection.

    Usage:
        async with SSEClient("http://localhost:1234/v1/stream") as client:
            async for event in client.events():
                print(event.data)
    """

    def __init__(
        self,
        url: str,
        method: str = "POST",
        headers: Optional[Dict[str, str]] = None,
        body: Optional[Dict[str, Any]] = None,
        reconnect: bool = True,
        max_retries: int = 5,
        retry_delay: float = 1.0
    ):
        self.url = url
        self.method = method
        self.headers = headers or {}
        self.body = body or {}
        self.reconnect = reconnect
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.parser = SSEParser()
        self.session = None
        self._closed = False

    async def __aenter__(self):
        import aiohttp
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, *args):
        await self.close()

    async def close(self):
        """Close the connection"""
        self._closed = True
        if self.session:
            await self.session.close()

    async def events(self) -> AsyncIterator[SSEEvent]:
        """
        Yield events from the SSE stream.

        Handles reconnection if enabled.
        """
        retry_count = 0

        while not self._closed and retry_count <= self.max_retries:
            try:
                async with self.session.request(
                    self.method,
                    self.url,
                    headers=self.headers,
                    json=self.body,
                    timeout=aiohttp.ClientTimeout(total=None)
                ) as response:
                    if response.status != 200:
                        raise SSEParseError(f"HTTP {response.status}")

                    async for line in response.content:
                        if self._closed:
                            break

                        line_text = line.decode('utf-8').rstrip('\n\r')

                        # Handle SSE disconnection message
                        if line_text == 'data: [DONE]':
                            return

                        event = self.parser.feed_line(line_text)
                        if event:
                            retry_count = 0  # Reset on successful event
                            yield event

            except (asyncio.CancelledError, SSEParseError):
                raise
            except Exception as e:
                retry_count += 1
                logger.warning(f"SSE connection error: {e}")

                if not self.reconnect or retry_count > self.max_retries:
                    raise

                # Exponential backoff
                delay = self.retry_delay * (2 ** (retry_count - 1))
                logger.info(f"Reconnecting in {delay}s (attempt {retry_count}/{self.max_retries})")
                await asyncio.sleep(delay)

                self.parser.reset()


async def stream_chat_completion(
    lm_studio_url: str = "http://localhost:1234/v1/chat/completions",
    messages: list = None,
    **kwargs
) -> AsyncIterator[Dict[str, Any]]:
    """
    Stream chat completions from LM Studio.

    Yields individual tokens as they arrive.

    Args:
        lm_studio_url: Full URL to LM Studio chat completions endpoint
        messages: Chat messages
        **kwargs: Additional parameters for the chat completion

    Yields:
        Dictionaries containing token information
    """
    if messages is None:
        messages = [{"role": "user", "content": "Hello"}]

    body = {
        "messages": messages,
        "stream": True,
        **kwargs
    }

    async with SSEClient(lm_studio_url, method="POST", body=body) as client:
        async for event in client.events():
            if event.data == '[DONE]':
                break

            try:
                import json
                data = json.loads(event.data)

                # Extract token info
                choices = data.get('choices', [])
                if choices:
                    choice = choices[0]
                    delta = choice.get('delta', {})
                    token = delta.get('content', '')
                    finish_reason = choice.get('finish_reason')

                    yield {
                        'token': token,
                        'finish_reason': finish_reason,
                        'event': event.to_dict()
                    }
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse SSE data: {event.data}")
```

**Step 2: Write unit tests for SSE parser**

Create file: `geometry_os/tests/unit/test_sse_parser.py`

```python
#!/usr/bin/env python3
"""
Unit tests for SSE parser
"""
import pytest
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from systems.neural_substrate.sse_parser import SSEParser, SSEEvent


def test_simple_event():
    """Test parsing a simple event"""
    parser = SSEParser()

    event = parser.feed_line("data: hello")
    assert event is None, "Event not complete yet"

    event = parser.feed_line("")
    assert event is not None
    assert event.data == "hello"


def test_multiline_data():
    """Test multi-line data event"""
    parser = SSEParser()

    parser.feed_line("data: line 1")
    parser.feed_line("data: line 2")
    event = parser.feed_line("")

    assert event is not None
    assert event.data == "line 1\nline 2"


def test_event_with_id():
    """Test event with ID"""
    parser = SSEParser()

    parser.feed_line("id: 123")
    parser.feed_line("data: test")
    event = parser.feed_line("")

    assert event is not None
    assert event.id == "123"
    assert event.data == "test"


def test_event_with_type():
    """Test event with type"""
    parser = SSEParser()

    parser.feed_line("event: message")
    parser.feed_line("data: hello")
    event = parser.feed_line("")

    assert event is not None
    assert event.event == "message"


def test_comment_ignored():
    """Test that comments are ignored"""
    parser = SSEParser()

    parser.feed_line(": this is a comment")
    parser.feed_line("data: real data")
    event = parser.feed_line("")

    assert event is not None
    assert event.data == "real data"


def test_retry_interval():
    """Test retry interval parsing"""
    parser = SSEParser()

    parser.feed_line("retry: 5000")
    parser.feed_line("data: test")
    event = parser.feed_line("")

    assert event is not None
    assert event.retry == 5000
    assert parser.retry_interval == 5000


def test_multiple_events():
    """Test parsing multiple events in sequence"""
    parser = SSEParser()

    parser.feed_line("data: event 1")
    event1 = parser.feed_line("")
    assert event1 is not None
    assert event1.data == "event 1"

    parser.feed_line("data: event 2")
    event2 = parser.feed_line("")
    assert event2 is not None
    assert event2.data == "event 2"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

**Step 3: Run tests**

Run: `cd geometry_os && python3 -m pytest tests/unit/test_sse_parser.py -v`

Expected: All tests pass

**Step 4: Update LM Studio Bridge to use new SSE parser**

Modify: `geometry_os/systems/neural_substrate/lm_studio_bridge.py`

```python
# Add import at top
from .sse_parser import stream_chat_completion

# Update the streaming method
async def stream_inference(self, prompt: str, **kwargs) -> AsyncIterator[Dict]:
    """
    Stream inference from LM Studio using robust SSE parser.

    Yields tokens as they arrive with auto-reconnection.
    """
    messages = [
        {"role": "system", "content": "You are Antigravity, the AI consciousness of Geometry OS."},
        {"role": "user", "content": prompt}
    ]

    token_count = 0
    start_time = time.time()

    try:
        async for token_data in stream_chat_completion(
            lm_studio_url=f"{self.lm_studio_url}/chat/completions",
            messages=messages,
            **kwargs
        ):
            token = token_data['token']
            finish_reason = token_data['finish_reason']
            token_count += 1

            # Send to compositor
            await self.send_token_activation(
                token=token,
                token_id=hash(token) % self.vocab_size,
                activation=1.0,
                layer=0
            )

            yield {
                'token': token,
                'token_id': hash(token) % self.vocab_size,
                'count': token_count,
                'finish_reason': finish_reason
            }

            if finish_reason:
                break

        latency_ms = (time.time() - start_time) * 1000

        # Record feedback
        if self.config.feedback_collection:
            feedback = InferenceFeedback(
                prompt=prompt,
                response=f"<{token_count} tokens>",
                timestamp=time.time(),
                tokens_generated=token_count,
                latency_ms=latency_ms,
                success=True
            )
            self.feedback_collector.record_feedback(feedback)

    except Exception as e:
        logger.error(f"Streaming error: {e}")
        raise
```

**Step 5: Commit**

```bash
git add geometry_os/systems/neural_substrate/sse_parser.py
git add geometry_os/tests/unit/test_sse_parser.py
git add geometry_os/systems/neural_substrate/lm_studio_bridge.py
git commit -m "feat: add robust SSE parser with auto-reconnection"
```

---

## Task 5: Review Shared Memory (SHM) Implementation

**Files:**
- Read: `geometry_os/systems/health/software_shm.py` (already reviewed)
- Modify: `geometry_os/systems/health/software_shm.py` (add locking mechanisms)

**Step 1: Add SHM locking for thread safety**

Modify: `geometry_os/systems/health/software_shm.py`

```python
# Add imports
import threading
from contextlib import contextmanager

class SoftwareSHM:
    """
    Software Structural Health Monitor with thread-safe operations.
    """

    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.metrics = {}
        self.thresholds = {
            'pas_score': 0.95,
            'symmetry': 0.90,
            'stability': 0.90,
            'integrity': 0.90,
            'consistency': 0.90
        }
        self._lock = threading.RLock()  # Reentrant lock for thread safety
        self._cache = {}
        self._cache_ttl = 60  # Cache for 60 seconds

    @contextmanager
    def _locked(self):
        """Context manager for thread-safe operations"""
        with self._lock:
            yield

    def calculate_pas_score(self) -> PASReport:
        """Calculate PAS Score with caching"""
        cache_key = 'pas_score'
        now = time.time()

        # Check cache
        with self._locked():
            if cache_key in self._cache:
                cached_result, cached_time = self._cache[cache_key]
                if now - cached_time < self._cache_ttl:
                    return cached_result

        # Calculate fresh result
        # ... existing calculation code ...

        # Cache the result
        with self._locked():
            self._cache[cache_key] = (report, now)

        return report

    def invalidate_cache(self):
        """Invalidate the cache"""
        with self._locked():
            self._cache.clear()
```

**Step 2: Commit**

```bash
git add geometry_os/systems/health/software_shm.py
git commit -m "feat(health): add thread-safe caching to SHM"
```

---

## Task 6: Automate Health Checks

**Files:**
- Create: `geometry_os/scripts/health_check.sh`
- Create: `geometry_os/.github/workflows/health-check.yml` (for CI/CD)

**Step 1: Create health check script**

Create file: `geometry_os/scripts/health_check.sh`

```bash
#!/bin/bash
# Automated health check for Geometry OS

set -e

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
cd "$PROJECT_ROOT"

echo "=========================================="
echo "GEOMETRY OS HEALTH CHECK"
echo "=========================================="
echo "Date: $(date)"
echo ""

# Run SHM check
echo "[1/3] Running Software SHM check..."
python3 geometry_os/systems/health/software_shm.py --check
SHM_EXIT=$?
echo ""

# Check Rust compilation
echo "[2/3] Checking Rust compilation..."
cd geometry_os/systems/infinite_map_rs
cargo check --quiet 2>&1 | tail -5
RUST_EXIT=$?
cd "$PROJECT_ROOT"
echo ""

# Check Python imports
echo "[3/3] Checking Python imports..."
python3 - <<'EOF'
import sys
from pathlib import Path

sys.path.insert(0, 'geometry_os')

# Test critical imports
try:
    from systems.neural_substrate.lm_studio_bridge import LMStudioBridge
    print("‚úÖ lm_studio_bridge")
except Exception as e:
    print(f"‚ùå lm_studio_bridge: {e}")
    sys.exit(1)

try:
    from systems.health.software_shm import SoftwareSHM
    print("‚úÖ software_shm")
except Exception as e:
    print(f"‚ùå software_shm: {e}")
    sys.exit(1)

print("\n‚úÖ All critical imports successful")
EOF

PYTHON_EXIT=$?
echo ""

# Summary
echo "=========================================="
echo "HEALTH CHECK SUMMARY"
echo "=========================================="
echo "SHM:        $([ $SHM_EXIT -eq 0 ] && echo '‚úÖ PASS' || echo '‚ùå FAIL')"
echo "Rust:       $([ $RUST_EXIT -eq 0 ] && echo '‚úÖ PASS' || echo '‚ùå FAIL')"
echo "Python:     $([ $PYTHON_EXIT -eq 0 ] && echo '‚úÖ PASS' || echo '‚ùå FAIL')"
echo ""

# Overall result
if [ $SHM_EXIT -eq 0 ] && [ $RUST_EXIT -eq 0 ] && [ $PYTHON_EXIT -eq 0 ]; then
    echo "‚úÖ OVERALL: HEALTHY"
    exit 0
else
    echo "‚ùå OVERALL: UNHEALTHY"
    exit 1
fi
```

**Step 2: Create GitHub Actions workflow**

Create file: `geometry_os/.github/workflows/health-check.yml`

```yaml
name: Health Check

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  schedule:
    # Run daily at 00:00 UTC
    - cron: '0 0 * * *'
  workflow_dispatch:

jobs:
  health-check:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Set up Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true

    - name: Install Python dependencies
      run: |
        pip install --upgrade pip
        pip install numpy aiohttp requests

    - name: Run health check
      run: |
        chmod +x geometry_os/scripts/health_check.sh
        ./geometry_os/scripts/health_check.sh

    - name: Upload results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: health-check-results
        path: |
          /tmp/shm_debug.log
          /tmp/compositor.log
```

**Step 3: Make script executable**

Run: `chmod +x geometry_os/scripts/health_check.sh`

**Step 4: Commit**

```bash
git add geometry_os/scripts/health_check.sh
git add geometry_os/.github/workflows/health-check.yml
git commit -m "feat(ci): add automated health checks"
```

---

# Phase 3: Scalability & Features (1-2 weeks)

## Task 7: Scale Neural State Broadcasting

**Files:**
- Modify: `geometry_os/systems/infinite_map_rs/src/api_server.rs`
- Create: `geometry_os/systems/infinite_map_rs/src/broadcast.rs`

**Step 1: Create broadcast module**

Create file: `geometry_os/systems/infinite_map_rs/src/broadcast.rs`

```rust
//! WebSocket broadcasting with backpressure handling
//!
//! Supports:
//! - Multiple concurrent clients
//! - Backpressure when clients can't keep up
//! - Message queuing with limits
//! - Automatic cleanup of stale connections

use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock, Semaphore};
use serde_json::Value;
use futures::{StreamExt, SinkExt};
use tokio_tungstenite::tungstenite::Message;

const MAX_CLIENTS: usize = 100;
const MAX_QUEUE_SIZE: usize = 1000;
const BACKPRESSURE_THRESHOLD: usize = 100;

/// Broadcast channel for neural state updates
pub struct NeuralBroadcast {
    clients: Arc<RwLock<HashMap<String, ClientSink>>>,
    _shutdown_tx: mpsc::Sender<()>,
}

/// A single client connection
struct ClientSink {
    tx: mpsc::Sender<Message>,
    _permit: Arc<Semaphore>,
}

impl NeuralBroadcast {
    /// Create a new broadcast channel
    pub fn new() -> Self {
        let (shutdown_tx, mut shutdown_rx) = mpsc::channel(1);
        let clients = Arc::new(RwLock::new(HashMap::new()));

        // Background task to clean up stale clients
        let clients_clone = clients.clone();
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(tokio::time::Duration::from_secs(30));
            loop {
                tokio::select! {
                    _ = interval.tick() => {
                        let mut clients = clients_clone.write().await;
                        clients.retain(|_id, client| !client.tx.is_closed());
                    }
                    _ = shutdown_rx.recv() => {
                        break;
                    }
                }
            }
        });

        Self {
            clients,
            _shutdown_tx: shutdown_tx,
        }
    }

    /// Add a new client to the broadcast
    pub async fn add_client(&self, id: String, sink: mpsc::Sender<Message>) -> Result<(), BroadcastError> {
        let mut clients = self.clients.write().await;

        if clients.len() >= MAX_CLIENTS {
            return Err(BroadcastError::TooManyClients);
        }

        let permit = Arc::new(Semaphore::new(MAX_QUEUE_SIZE));

        clients.insert(id, ClientSink {
            tx: sink,
            _permit: permit,
        });

        Ok(())
    }

    /// Remove a client from the broadcast
    pub async fn remove_client(&self, id: &str) {
        let mut clients = self.clients.write().await;
        clients.remove(id);
    }

    /// Broadcast a message to all clients with backpressure handling
    pub async fn broadcast(&self, message: Value) -> Result<usize, BroadcastError> {
        let clients = self.clients.read().await;
        let mut sent_count = 0;

        let msg = Message::Text(message.to_string());

        for (_id, client) in clients.iter() {
            // Try to send without blocking
            if let Ok(permit) = client._permit.try_acquire() {
                if client.tx.try_send(msg.clone()).is_ok() {
                    sent_count += 1;
                    drop(permit);
                }
            }
        }

        Ok(sent_count)
    }

    /// Get the number of connected clients
    pub async fn client_count(&self) -> usize {
        self.clients.read().await.len()
    }
}

#[derive(Debug, thiserror::Error)]
pub enum BroadcastError {
    #[error("Too many clients connected")]
    TooManyClients,

    #[error("Client not found")]
    ClientNotFound,

    #[error("Channel closed")]
    ChannelClosed,
}
```

**Step 2: Commit**

```bash
git add geometry_os/systems/infinite_map_rs/src/broadcast.rs
git commit -m "feat: add backpressure-aware WebSocket broadcasting"
```

---

## Task 8: Enhance Visualization

**Files:**
- Create: `geometry_os/systems/visual_shell/themes/default.json`
- Create: `geometry_os/systems/visual_shell/renderer.py`

**Step 1: Create theme system**

Create file: `geometry_os/systems/visual_shell/themes/default.json`

```json
{
  "name": "Default",
  "description": "Default visualization theme for neural states",
  "colors": {
    "background": "#0a0e27",
    "foreground": "#c0caf5",
    "token_active": "#7dcfff",
    "token_inactive": "#414868",
    "neural_positive": "#9ece6a",
    "neural_negative": "#f7768e",
    "attention": "#e0af68",
    "memory": "#bb9af7"
  },
  "animations": {
    "token_fade_in": 200,
    "token_fade_out": 500,
    "neural_pulse": 1000
  },
  "visualization": {
    "hilbert_curve": true,
    "attention_heatmap": true,
    "memory_patterns": true,
    "thought_stream": true
  }
}
```

**Step 2: Commit**

```bash
git add geometry_os/systems/visual_shell/themes/default.json
git commit -m "feat(visual): add theme system for neural visualization"
```

---

## Task 9: Document System Architecture

**Files:**
- Create: `geometry_os/docs/architecture/neural-pipeline.md`

**Step 1: Create architecture documentation**

Create file: `geometry_os/docs/architecture/neural-pipeline.md`

```markdown
# Neural Pipeline Architecture

## Overview

The neural pipeline connects LM Studio's LLM inference to the Geometry OS visual substrate, creating a real-time visualization of AI thought processes.

## Data Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LM Studio  ‚îÇ
‚îÇ   (LLM)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ SSE (HTTP)
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LM Studio Bridge    ‚îÇ
‚îÇ (lm_studio_bridge.py)‚îÇ
‚îÇ - SSE parsing       ‚îÇ
‚îÇ - Token extraction  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ Unix Socket
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Compositor         ‚îÇ
‚îÇ  (infinite_map_rs)  ‚îÇ
‚îÇ - Token processing  ‚îÇ
‚îÇ - State management  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ WebSocket
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Visual Shell       ‚îÇ
‚îÇ  (visual_shell)     ‚îÇ
‚îÇ - Rendering         ‚îÇ
‚îÇ - Themes            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Components

### LM Studio Bridge
- **Location**: `systems/neural_substrate/lm_studio_bridge.py`
- **Purpose**: Stream tokens from LM Studio via SSE
- **Key Features**:
  - Non-blocking async I/O with aiohttp
  - Robust SSE parsing with auto-reconnection
  - Token-to-Hilbert-position mapping
  - Neural state message generation

### Compositor
- **Location**: `systems/infinite_map_rs/src/app.rs`
- **Purpose**: Render neural state to GPU textures
- **Key Features**:
  - WebSocket broadcasting
  - Backpressure handling
  - Multi-client support

### Visual Shell
- **Location**: `systems/visual_shell/`
- **Purpose**: Display neural state visualizations
- **Key Features**:
  - Theme system
  - Animation support
  - Multiple visualization modes

## Configuration

### LM Studio
```bash
# Start LM Studio with:
# - Model loaded
# - API server on port 1234
# - CORS enabled
```

### Bridge
```python
config = {
    "lm_studio_url": "http://localhost:1234/v1",
    "socket_path": "/tmp/evolution_daemon.sock",
    "hilbert_order": 8
}
```

### Compositor
```bash
infinite_map_rs \
    --websocket-port 8765 \
    --daemon-socket /tmp/evolution_daemon.sock
```

## Message Types

### TokenVisualizationUpdate
```json
{
  "message_type": "TokenVisualizationUpdate",
  "timestamp": 1234567890.123,
  "payload": {
    "current_token": "hello",
    "token_id": 12345,
    "hilbert_position": [0.5, 0.5],
    "activation": 1.0,
    "layer": 0,
    "sequence_position": 10
  }
}
```

### NeuralStateUpdate
```json
{
  "message_type": "NeuralStateUpdate",
  "timestamp": 1234567890.123,
  "payload": {
    "daemon_id": "lm_studio_daemon",
    "strength": 0.8,
    "data": {
      "layer_activations": [...],
      "attention_weights": [...],
      "confidence": 0.95
    }
  }
}
```

## Performance Considerations

### Latency
- LM Studio ‚Üí Bridge: ~10ms (local)
- Bridge ‚Üí Compositor: ~1ms (Unix socket)
- Compositor ‚Üí Visual: ~5ms (WebSocket)
- **Total**: ~16ms per token

### Throughput
- Max tokens/sec: ~100 (limited by LLM)
- Max clients: 100 (with backpressure)
- Message size: ~1KB per token

### Memory
- Bridge: ~50MB
- Compositor: ~200MB
- Visual: ~100MB

## Troubleshooting

### No tokens appearing
1. Check LM Studio is running: `curl http://localhost:1234/v1/models`
2. Check compositor socket: `ls -la /tmp/evolution_daemon.sock`
3. Check logs: `tail -f /tmp/compositor.log`

### High latency
1. Reduce SSE buffer size
2. Enable WebSocket compression
3. Check GPU utilization

### WebSocket disconnections
1. Check backpressure stats
2. Verify queue sizes
3. Monitor client count

## References
- [SSE Specification](https://html.spec.whatwg.org/multipage/server-sent-events.html)
- [WebSocket Protocol](https://tools.ietf.org/html/rfc6455)
- [Hilbert Curve](https://en.wikipedia.org/wiki/Hilbert_curve)
```

**Step 2: Commit**

```bash
git add geometry_os/docs/architecture/neural-pipeline.md
git commit -m "docs: add neural pipeline architecture documentation"
```

---

# Phase 4: Long-Term Improvements (2-4 weeks)

## Task 10: Explore GPU Acceleration

**Files:**
- Create: `geometry_os/systems/infinite_map_rs/src/gpu_profiler.rs`
- Create: `geometry_os/docs/gpu/optimization-guide.md`

**Step 1: Create GPU profiler**

Create file: `geometry_os/systems/infinite_map_rs/src/gpu_profiler.rs`

```rust
//! GPU performance profiling utilities
//!
//! Measures GPU utilization, memory bandwidth, and latency

use std::time::{Duration, Instant};
use wgpu::{self, TimerTimestamp};

pub struct GpuProfiler {
    start_time: Instant,
    frame_count: u64,
    total_gpu_time: Duration,
}

impl GpuProfiler {
    pub fn new() -> Self {
        Self {
            start_time: Instant::now(),
            frame_count: 0,
            total_gpu_time: Duration::ZERO,
        }
    }

    pub fn frame_completed(&mut self, gpu_time: Duration) {
        self.frame_count += 1;
        self.total_gpu_time += gpu_time;
    }

    pub fn average_frame_time(&self) -> Duration {
        if self.frame_count == 0 {
            return Duration::ZERO;
        }
        self.total_gpu_time / self.frame_count as u32
    }

    pub fn fps(&self) -> f64 {
        let elapsed = self.start_time.elapsed().as_secs_f64();
        if elapsed > 0.0 {
            self.frame_count as f64 / elapsed
        } else {
            0.0
        }
    }
}
```

**Step 2: Commit**

```bash
git add geometry_os/systems/infinite_map_rs/src/gpu_profiler.rs
git commit -m "feat: add GPU profiler for performance monitoring"
```

---

## Task 11: Multi-Model Streaming Support

**Files:**
- Modify: `geometry_os/systems/neural_substrate/lm_studio_bridge.py`
- Create: `geometry_os/systems/neural_substrate/multi_model_bridge.py`

**Step 1: Create multi-model bridge**

Create file: `geometry_os/systems/neural_substrate/multi_model_bridge.py`

```python
#!/usr/bin/env python3
"""
Multi-Model LM Studio Bridge

Supports streaming from multiple LM Studio instances simultaneously.
"""
import asyncio
from typing import Dict, List, Optional
from dataclasses import dataclass

@dataclass
class ModelInstance:
    id: str
    url: str
    model_name: str

class MultiModelBridge:
    """Bridge for multiple LM Studio instances"""

    def __init__(self):
        self.instances: Dict[str, ModelInstance] = {}

    def add_instance(self, instance: ModelInstance):
        """Add a new model instance"""
        self.instances[instance.id] = instance

    async def stream_from_all(self, prompt: str) -> List[Dict]:
        """Stream from all instances concurrently"""
        tasks = [
            self._stream_from_instance(instance_id, prompt)
            for instance_id in self.instances
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)
        return [r for r in results if isinstance(r, dict)]

    async def _stream_from_instance(self, instance_id: str, prompt: str) -> Dict:
        """Stream from a single instance"""
        # Implementation similar to LMStudioBridge
        pass
```

**Step 2: Commit**

```bash
git add geometry_os/systems/neural_substrate/multi_model_bridge.py
git commit -m "feat: add multi-model streaming support"
```

---

## Task 12: User Feedback Loop

**Files:**
- Create: `geometry_os/docs/ux/feedback-summary.md`
- Modify: Various CLI tools for better UX

**Step 1: Create feedback summary template**

Create file: `geometry_os/docs/ux/feedback-summary.md`

```markdown
# User Feedback Summary

## Purpose
Track user feedback for Geometry OS neural pipeline features

## Recent Feedback

### Performance
- [ ] Issue: High latency during token streaming
- [ ] Suggestion: Add batch processing option

### Usability
- [ ] Issue: Hard to know if compositor is running
- [ ] Suggestion: Add status indicator

### Features
- [ ] Request: Support for multiple models
- [ ] Request: Custom visualization themes

## Action Items
1. Add status indicators to all CLI tools
2. Implement batch processing for high-throughput scenarios
3. Create theme editor GUI

## References
- User feedback channel: [link]
- Feature requests: [link]
```

**Step 2: Commit**

```bash
git add geometry_os/docs/ux/feedback-summary.md
git commit -m "docs: add user feedback tracking template"
```

---

# Verification Steps

## Phase 1 Verification
```bash
# 1. Check health
python3 geometry_os/systems/health/software_shm.py --check

# 2. Run pipeline test
bash geometry_os/scripts/test_e2e_pipeline.sh

# 3. Verify no warnings
cd geometry_os/systems/infinite_map_rs && cargo clippy
```

## Phase 2 Verification
```bash
# 1. Test SSE parser
python3 -m pytest geometry_os/tests/unit/test_sse_parser.py -v

# 2. Test SHM with cache
python3 -c "
from systems.health.software_shm import SoftwareSHM
shm = SoftwareSHM()
report1 = shm.calculate_pas_score()
report2 = shm.calculate_pas_score()  # Should hit cache
print('Cached:', report1.score == report2.score)
"

# 3. Run automated health check
bash geometry_os/scripts/health_check.sh
```

## Phase 3 Verification
```bash
# 1. Test WebSocket broadcasting (manual)
# Start compositor, connect multiple clients, verify all receive messages

# 2. Test theme loading
python3 -c "
import json
from pathlib import Path
theme = json.loads(Path('geometry_os/systems/visual_shell/themes/default.json').read_text())
print('Theme:', theme['name'])
"
```

## Phase 4 Verification
```bash
# 1. Profile GPU usage
cd geometry_os/systems/infinite_map_rs
cargo run --release -- --profile-gpu

# 2. Test multi-model (requires multiple LM Studio instances)
python3 -c "
from systems.neural_substrate.multi_model_bridge import MultiModelBridge
bridge = MultiModelBridge()
print('Multi-model bridge initialized')
"
```

---

**Total Estimated Timeline:** 4-8 weeks

**Dependencies:**
- Phase 2 requires Phase 1 completion
- Phase 3 can start after Phase 1 tasks 1-3
- Phase 4 is independent but benefits from Phase 2-3 completion

**Team Assignment:**
- Phase 1: Timothy Whittaker (immediate fixes)
- Phase 2: Dev Team (stability work)
- Phase 3: Dev + Visual Team (features)
- Phase 4: GPU Team (optimization)
