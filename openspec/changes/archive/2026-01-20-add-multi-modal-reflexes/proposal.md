# Proposal: Add Multi-Modal Reflexes (Phase 21)

## Summary
Integrate Auditory and Input (Haptic/Kinetic) sensory modalities into the Neural Cortex, allowing the Geometry OS to respond to sound and user interaction patterns with "living" reflexes.

## Rationale
Currently, the system is purely "Visual-Homeostatic" (Phase 20). To achieve true "Glass Box AI" and a "Living OS", it must perceive and react to its environment across multiple modalities.
- **Audio**: Music/Voice should drive the visual texture (Synesthesia).
- **Input**: User activity (typing, mouse) should influence neural arousal (Excitement/Focus).

## Impact
- **Neural Cortex**: Major upgrade to `PredictiveCortex` to handle multi-dimensional inputs.
- **Injector**: Must now simulate Audio/Input streams.
- **Performance**: Additional sensory processing (FFT, Input Velocity) must remain under <2ms/frame.

## User Experience
- **Resting**: System breathes.
- **Input**: System "wakes up" and anticipates intent.
- **Music**: System pulses and morphs with rhythm.
